{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ef2b38-f450-4afc-a218-4e9d6a8b6779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c0e71a-003f-4be0-8d63-35688ddb4bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805f9509-f343-4807-83c7-eb59cdc72c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parents[1]))\n",
    "from models.resnet32_model import ResNetQAT\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf5c230e-1c18-4722-8d32-f743de4620b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetQAT(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.ao.quantization as tq\n",
    "\n",
    "model = ResNetQAT(num_classes=10)\n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e70f2b-a5ce-4d1b-b356-83c0559ecb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing: 0\n",
      "unexpected: 0\n",
      "example missing: []\n",
      "example unexpected: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNetQAT(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "def strip_and_remap_qat_state(ckpt: dict) -> dict:\n",
    "    out = {}\n",
    "\n",
    "    for k, v in ckpt.items():\n",
    "        # 1) drop all QAT bookkeeping\n",
    "        if (\n",
    "            \"activation_post_process\" in k\n",
    "            or \"fake_quant\" in k\n",
    "            or \"weight_fake_quant\" in k\n",
    "            or k.startswith(\"activation_post_process_\")\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        nk = k\n",
    "\n",
    "        # 2) stem BN\n",
    "        nk = nk.replace(\"conv1.bn.\", \"bn1.\")\n",
    "\n",
    "        # 3) residual blocks\n",
    "        nk = re.sub(r\"(layer\\d+\\.\\d+)\\.conv1\\.bn\\.\", r\"\\1.bn1.\", nk)\n",
    "        nk = re.sub(r\"(layer\\d+\\.\\d+)\\.conv2\\.bn\\.\", r\"\\1.bn2.\", nk)\n",
    "\n",
    "        # 4) shortcut BN (conv, bn)\n",
    "        nk = re.sub(r\"(layer\\d+\\.\\d+)\\.shortcut\\.0\\.bn\\.\", r\"\\1.shortcut.1.\", nk)\n",
    "\n",
    "        out[nk] = v\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---- LOAD ----\n",
    "ckpt = torch.load(\"../../pth/resnet_qat_preconvert.pth\", map_location=\"cpu\")\n",
    "state_fp32 = strip_and_remap_qat_state(ckpt)\n",
    "\n",
    "missing, unexpected = model.load_state_dict(state_fp32, strict=False)\n",
    "print(\"missing:\", len(missing))\n",
    "print(\"unexpected:\", len(unexpected))\n",
    "print(\"example missing:\", missing[:20])\n",
    "print(\"example unexpected:\", unexpected[:20])\n",
    "\n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c983ce95-b2e5-4fdf-bdc3-e3fbe72eb482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch acc: 88.64\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def torch_acc(model, loader, device=device):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "print(\"Torch acc:\", torch_acc(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bfea1ce-3bf5-4c9d-8ffc-c68fc428cba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 66 of general pattern rewrite rules.\n",
      "Exported: resnet_qat_qdq_b1_op18.onnx\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 66 of general pattern rewrite rules.\n",
      "Exported: resnet_qat_qdq_b64_op18.onnx\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 66 of general pattern rewrite rules.\n",
      "Exported: resnet_qat_qdq_b128_op18.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval().to(device)\n",
    "\n",
    "dummy_map = {\n",
    "    1:   torch.randn(1,   3, 32, 32, device=device),\n",
    "    64:  torch.randn(64,  3, 32, 32, device=device),\n",
    "    128: torch.randn(128, 3, 32, 32, device=device),\n",
    "}\n",
    "\n",
    "for bs, dummy in dummy_map.items():\n",
    "    out_path = f\"resnet_qat_qdq_b{bs}_op18.onnx\"\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy,\n",
    "        out_path,\n",
    "        opset_version=18,          # ✅ REQUIRED for Q/DQ\n",
    "        do_constant_folding=False, # ✅ MUST be False for QAT\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes=None,         # ✅ STATIC shapes\n",
    "    )\n",
    "\n",
    "    print(\"Exported:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efd387c9-11eb-4c8e-a414-0a0dd9300f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 84K Dec 14 11:12 resnet_qat_qdq_b1_op18.onnx\n",
      "-rw-r--r-- 1 ihsiao ihsiao 84K Dec 14 11:12 resnet_qat_qdq_b64_op18.onnx\n",
      "-rw-r--r-- 1 ihsiao ihsiao 84K Dec 14 11:12 resnet_qat_qdq_b128_op18.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -lh resnet_qat_qdq_b1_op18.onnx\n",
    "!ls -lh resnet_qat_qdq_b64_op18.onnx\n",
    "!ls -lh resnet_qat_qdq_b128_op18.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4782b1d-c490-4e17-9a75-575c3e84becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2025-11:29:16] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 612, GPU 3047 (MiB)\n",
      "[12/14/2025-11:29:16] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-11:29:16] [TRT] [I] ONNX IR version:  0.0.10\n",
      "[12/14/2025-11:29:16] [TRT] [I] Opset version:    18\n",
      "[12/14/2025-11:29:16] [TRT] [I] Producer name:    pytorch\n",
      "[12/14/2025-11:29:16] [TRT] [I] Producer version: 2.9.1+cu128\n",
      "[12/14/2025-11:29:16] [TRT] [I] Domain:           \n",
      "[12/14/2025-11:29:16] [TRT] [I] Model version:    0\n",
      "[12/14/2025-11:29:16] [TRT] [I] Doc string:       \n",
      "[12/14/2025-11:29:16] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-11:29:17] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +287, GPU +6, now: CPU 1100, GPU 3053 (MiB)\n",
      "[12/14/2025-11:29:17] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-11:29:33] [TRT] [I] Compiler backend is used during engine build.\n",
      "[12/14/2025-11:29:40] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-11:29:41] [TRT] [I] Total Host Persistent Memory: 184368 bytes\n",
      "[12/14/2025-11:29:41] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/14/2025-11:29:41] [TRT] [I] Max Scratch Memory: 512 bytes\n",
      "[12/14/2025-11:29:41] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 37 steps to complete.\n",
      "[12/14/2025-11:29:41] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.248417ms to assign 4 blocks to 37 nodes requiring 98816 bytes.\n",
      "[12/14/2025-11:29:41] [TRT] [I] Total Activation Memory: 98304 bytes\n",
      "[12/14/2025-11:29:41] [TRT] [I] Total Weights Memory: 946964 bytes\n",
      "[12/14/2025-11:29:41] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[12/14/2025-11:29:41] [TRT] [I] Engine generation completed in 24.6224 seconds.\n",
      "[12/14/2025-11:29:41] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 5 MiB\n",
      "Saved: resnet_qat_qdq_b1.engine\n",
      "[12/14/2025-11:29:42] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-11:29:42] [TRT] [I] ONNX IR version:  0.0.10\n",
      "[12/14/2025-11:29:42] [TRT] [I] Opset version:    18\n",
      "[12/14/2025-11:29:42] [TRT] [I] Producer name:    pytorch\n",
      "[12/14/2025-11:29:42] [TRT] [I] Producer version: 2.9.1+cu128\n",
      "[12/14/2025-11:29:42] [TRT] [I] Domain:           \n",
      "[12/14/2025-11:29:42] [TRT] [I] Model version:    0\n",
      "[12/14/2025-11:29:42] [TRT] [I] Doc string:       \n",
      "[12/14/2025-11:29:42] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-11:29:45] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +0, now: CPU 3197, GPU 3307 (MiB)\n",
      "[12/14/2025-11:29:45] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-11:30:00] [TRT] [I] Compiler backend is used during engine build.\n",
      "[12/14/2025-11:30:06] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-11:30:06] [TRT] [I] Total Host Persistent Memory: 183856 bytes\n",
      "[12/14/2025-11:30:06] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/14/2025-11:30:06] [TRT] [I] Max Scratch Memory: 1536 bytes\n",
      "[12/14/2025-11:30:06] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 36 steps to complete.\n",
      "[12/14/2025-11:30:06] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.235463ms to assign 3 blocks to 36 nodes requiring 6291456 bytes.\n",
      "[12/14/2025-11:30:06] [TRT] [I] Total Activation Memory: 6291456 bytes\n",
      "[12/14/2025-11:30:06] [TRT] [I] Total Weights Memory: 947328 bytes\n",
      "[12/14/2025-11:30:06] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[12/14/2025-11:30:06] [TRT] [I] Engine generation completed in 21.5562 seconds.\n",
      "[12/14/2025-11:30:06] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 13 MiB\n",
      "Saved: resnet_qat_qdq_b64.engine\n",
      "[12/14/2025-11:30:07] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-11:30:07] [TRT] [I] ONNX IR version:  0.0.10\n",
      "[12/14/2025-11:30:07] [TRT] [I] Opset version:    18\n",
      "[12/14/2025-11:30:07] [TRT] [I] Producer name:    pytorch\n",
      "[12/14/2025-11:30:07] [TRT] [I] Producer version: 2.9.1+cu128\n",
      "[12/14/2025-11:30:07] [TRT] [I] Domain:           \n",
      "[12/14/2025-11:30:07] [TRT] [I] Model version:    0\n",
      "[12/14/2025-11:30:07] [TRT] [I] Doc string:       \n",
      "[12/14/2025-11:30:07] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-11:30:11] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +0, now: CPU 3199, GPU 3309 (MiB)\n",
      "[12/14/2025-11:30:11] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-11:30:26] [TRT] [I] Compiler backend is used during engine build.\n",
      "[12/14/2025-11:30:34] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-11:30:35] [TRT] [I] Total Host Persistent Memory: 182640 bytes\n",
      "[12/14/2025-11:30:35] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/14/2025-11:30:35] [TRT] [I] Max Scratch Memory: 2560 bytes\n",
      "[12/14/2025-11:30:35] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 36 steps to complete.\n",
      "[12/14/2025-11:30:35] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.225494ms to assign 3 blocks to 36 nodes requiring 12582912 bytes.\n",
      "[12/14/2025-11:30:35] [TRT] [I] Total Activation Memory: 12582912 bytes\n",
      "[12/14/2025-11:30:35] [TRT] [I] Total Weights Memory: 947328 bytes\n",
      "[12/14/2025-11:30:35] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[12/14/2025-11:30:35] [TRT] [I] Engine generation completed in 24.4086 seconds.\n",
      "[12/14/2025-11:30:35] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 25 MiB\n",
      "Saved: resnet_qat_qdq_b128.engine\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "\n",
    "onnx_map = {\n",
    "    1:   \"resnet_qat_qdq_b1_op18.onnx\",\n",
    "    64:  \"resnet_qat_qdq_b64_op18.onnx\",\n",
    "    128: \"resnet_qat_qdq_b128_op18.onnx\",\n",
    "}\n",
    "\n",
    "def build_qdq_engine(onnx_path, engine_path, workspace_bytes=(1 << 30)):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "        with open(onnx_path, \"rb\") as f:\n",
    "            if not parser.parse(f.read()):\n",
    "                for i in range(parser.num_errors):\n",
    "                    print(parser.get_error(i))\n",
    "                raise RuntimeError(f\"ONNX parse failed for {onnx_path}\")\n",
    "\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace_bytes)\n",
    "\n",
    "        # Optional but usually helpful fallback:\n",
    "        # (If some QDQ layers can't run int8, TRT can fall back to FP16)\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "\n",
    "        serialized = builder.build_serialized_network(network, config)\n",
    "        if serialized is None:\n",
    "            raise RuntimeError(f\"Engine build failed for {onnx_path}\")\n",
    "\n",
    "        with open(engine_path, \"wb\") as f:\n",
    "            f.write(serialized)\n",
    "\n",
    "    print(\"Saved:\", engine_path)\n",
    "\n",
    "for bs, onnx_path in onnx_map.items():\n",
    "    build_qdq_engine(onnx_path, f\"resnet_qat_qdq_b{bs}.engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e79f72-df0b-43c7-864f-2c95526aa779",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1   = trt_accuracy_static(\"resnet_qat_qdq_b1.engine\",   test_loader_b1)\n",
    "acc64  = trt_accuracy_static(\"resnet_qat_qdq_b64.engine\",  test_loader_b64)\n",
    "acc128 = trt_accuracy_static(\"resnet_qat_qdq_b128.engine\", test_loader_b128)\n",
    "\n",
    "print(f\"INT8 TRT Acc b1:   {acc1:.2f}%\")\n",
    "print(f\"INT8 TRT Acc b64:  {acc64:.2f}%\")\n",
    "print(f\"INT8 TRT Acc b128: {acc128:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7920f64d-8066-414b-b225-3ac2f5ff2740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantizeLinear: False\n",
      "DequantizeLinear: False\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "m = onnx.load(out_onnx)\n",
    "ops = set(n.op_type for n in m.graph.node)\n",
    "print(\"QuantizeLinear:\", \"QuantizeLinear\" in ops)\n",
    "print(\"DequantizeLinear:\", \"DequantizeLinear\" in ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af714ce-1796-4369-91c5-e01eeafedcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num keys: 802\n",
      "first 40 keys:\n",
      "  activation_post_process_0.fake_quant_enabled\n",
      "  activation_post_process_0.observer_enabled\n",
      "  activation_post_process_0.scale\n",
      "  activation_post_process_0.zero_point\n",
      "  activation_post_process_0.activation_post_process.eps\n",
      "  activation_post_process_0.activation_post_process.min_val\n",
      "  activation_post_process_0.activation_post_process.max_val\n",
      "  conv1.weight\n",
      "  conv1.bn.weight\n",
      "  conv1.bn.bias\n",
      "  conv1.bn.running_mean\n",
      "  conv1.bn.running_var\n",
      "  conv1.bn.num_batches_tracked\n",
      "  conv1.weight_fake_quant.fake_quant_enabled\n",
      "  conv1.weight_fake_quant.observer_enabled\n",
      "  conv1.weight_fake_quant.scale\n",
      "  conv1.weight_fake_quant.zero_point\n",
      "  conv1.weight_fake_quant.activation_post_process.eps\n",
      "  conv1.weight_fake_quant.activation_post_process.min_val\n",
      "  conv1.weight_fake_quant.activation_post_process.max_val\n",
      "  activation_post_process_1.fake_quant_enabled\n",
      "  activation_post_process_1.observer_enabled\n",
      "  activation_post_process_1.scale\n",
      "  activation_post_process_1.zero_point\n",
      "  activation_post_process_1.activation_post_process.eps\n",
      "  activation_post_process_1.activation_post_process.min_val\n",
      "  activation_post_process_1.activation_post_process.max_val\n",
      "  layer1.0.conv1.weight\n",
      "  layer1.0.conv1.bn.weight\n",
      "  layer1.0.conv1.bn.bias\n",
      "  layer1.0.conv1.bn.running_mean\n",
      "  layer1.0.conv1.bn.running_var\n",
      "  layer1.0.conv1.bn.num_batches_tracked\n",
      "  layer1.0.conv1.weight_fake_quant.fake_quant_enabled\n",
      "  layer1.0.conv1.weight_fake_quant.observer_enabled\n",
      "  layer1.0.conv1.weight_fake_quant.scale\n",
      "  layer1.0.conv1.weight_fake_quant.zero_point\n",
      "  layer1.0.conv1.weight_fake_quant.activation_post_process.eps\n",
      "  layer1.0.conv1.weight_fake_quant.activation_post_process.min_val\n",
      "  layer1.0.conv1.weight_fake_quant.activation_post_process.max_val\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ckpt = torch.load(\"../../pth/resnet_qat_preconvert.pth\", map_location=\"cpu\")\n",
    "print(\"num keys:\", len(ckpt))\n",
    "print(\"first 40 keys:\")\n",
    "for k in list(ckpt.keys())[:40]:\n",
    "    print(\" \", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdc83341-2d32-4830-988e-6cea09e18194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing: 132\n",
      "unexpected: 767\n",
      "example missing: ['bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var']\n",
      "example unexpected: ['activation_post_process_0.fake_quant_enabled', 'activation_post_process_0.observer_enabled', 'activation_post_process_0.scale', 'activation_post_process_0.zero_point', 'activation_post_process_0.activation_post_process.eps', 'activation_post_process_0.activation_post_process.min_val', 'activation_post_process_0.activation_post_process.max_val', 'activation_post_process_1.fake_quant_enabled', 'activation_post_process_1.observer_enabled', 'activation_post_process_1.scale', 'activation_post_process_1.zero_point', 'activation_post_process_1.activation_post_process.eps', 'activation_post_process_1.activation_post_process.min_val', 'activation_post_process_1.activation_post_process.max_val', 'activation_post_process_2.fake_quant_enabled', 'activation_post_process_2.observer_enabled', 'activation_post_process_2.scale', 'activation_post_process_2.zero_point', 'activation_post_process_2.activation_post_process.eps', 'activation_post_process_2.activation_post_process.min_val']\n"
     ]
    }
   ],
   "source": [
    "missing, unexpected = model.load_state_dict(ckpt, strict=False)\n",
    "print(\"missing:\", len(missing))\n",
    "print(\"unexpected:\", len(unexpected))\n",
    "print(\"example missing:\", missing[:20])\n",
    "print(\"example unexpected:\", unexpected[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170064d-9517-4379-9e55-20139390dc46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee599 (PyTorch)",
   "language": "python",
   "name": "ee599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
