{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ef2b38-f450-4afc-a218-4e9d6a8b6779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c0e71a-003f-4be0-8d63-35688ddb4bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e368294b-428b-4b99-99fb-6c65f5b60a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader_b1   = DataLoader(test_dataset, batch_size=1,   shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\n",
    "test_loader_b64  = DataLoader(test_dataset, batch_size=64,  shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\n",
    "test_loader_b128 = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "805f9509-f343-4807-83c7-eb59cdc72c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parents[1]))\n",
    "from models.resnet32_model import ResNetQAT\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf5c230e-1c18-4722-8d32-f743de4620b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetQAT(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.ao.quantization as tq\n",
    "from models.resnet32_model import ResNetQAT\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ckpt = \"../../pth/resnet_qat_preconvert.pth\"\n",
    "\n",
    "model = ResNetQAT(num_classes=10).to(device)\n",
    "state = torch.load(ckpt, map_location=\"cpu\")\n",
    "\n",
    "model.load_state_dict(state, strict=False)  # strict=False is fine here\n",
    "model.eval()\n",
    "\n",
    "# IMPORTANT for Q/DQ export:\n",
    "model.apply(tq.disable_observer)   # stops moving-average observer updates\n",
    "model.apply(tq.enable_fake_quant)  # keeps fake-quant so exporter emits Q/DQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84e70f2b-a5ce-4d1b-b356-83c0559ecb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing: 0\n",
      "unexpected: 0\n",
      "example missing: []\n",
      "example unexpected: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNetQAT(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "def strip_and_remap_qat_state(ckpt: dict) -> dict:\n",
    "    out = {}\n",
    "\n",
    "    for k, v in ckpt.items():\n",
    "        # 1) drop all QAT bookkeeping\n",
    "        if (\n",
    "            \"activation_post_process\" in k\n",
    "            or \"fake_quant\" in k\n",
    "            or \"weight_fake_quant\" in k\n",
    "            or k.startswith(\"activation_post_process_\")\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        nk = k\n",
    "\n",
    "        # 2) stem BN\n",
    "        nk = nk.replace(\"conv1.bn.\", \"bn1.\")\n",
    "\n",
    "        # 3) residual blocks\n",
    "        nk = re.sub(r\"(layer\\d+\\.\\d+)\\.conv1\\.bn\\.\", r\"\\1.bn1.\", nk)\n",
    "        nk = re.sub(r\"(layer\\d+\\.\\d+)\\.conv2\\.bn\\.\", r\"\\1.bn2.\", nk)\n",
    "\n",
    "        # 4) shortcut BN (conv, bn)\n",
    "        nk = re.sub(r\"(layer\\d+\\.\\d+)\\.shortcut\\.0\\.bn\\.\", r\"\\1.shortcut.1.\", nk)\n",
    "\n",
    "        out[nk] = v\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---- LOAD ----\n",
    "ckpt = torch.load(\"../../pth/resnet_qat_preconvert.pth\", map_location=\"cpu\")\n",
    "state_fp32 = strip_and_remap_qat_state(ckpt)\n",
    "\n",
    "missing, unexpected = model.load_state_dict(state_fp32, strict=False)\n",
    "print(\"missing:\", len(missing))\n",
    "print(\"unexpected:\", len(unexpected))\n",
    "print(\"example missing:\", missing[:20])\n",
    "print(\"example unexpected:\", unexpected[:20])\n",
    "\n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c983ce95-b2e5-4fdf-bdc3-e3fbe72eb482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch acc: 88.64\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def torch_acc(model, loader, device=device):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "print(\"Torch acc:\", torch_acc(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3771e1bd-da1d-47d0-bc6e-fbe8057b3ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetQAT(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.ao.quantization as tq\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Freeze QAT bookkeeping so it doesn't try to update moving averages during export\n",
    "model.apply(tq.disable_observer)     # ✅ stops moving_avg observer updates\n",
    "model.apply(tq.enable_fake_quant)    # ✅ keeps fake-quant so Q/DQ gets emitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bfea1ce-3bf5-4c9d-8ffc-c68fc428cba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 66 of general pattern rewrite rules.\n",
      "Exported: resnet_qat_qdq_b1_op18.onnx\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 66 of general pattern rewrite rules.\n",
      "Exported: resnet_qat_qdq_b64_op18.onnx\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 66 of general pattern rewrite rules.\n",
      "Exported: resnet_qat_qdq_b128_op18.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval().to(device)\n",
    "\n",
    "dummy_map = {\n",
    "    1:   torch.randn(1,   3, 32, 32, device=device),\n",
    "    64:  torch.randn(64,  3, 32, 32, device=device),\n",
    "    128: torch.randn(128, 3, 32, 32, device=device),\n",
    "}\n",
    "\n",
    "for bs, dummy in dummy_map.items():\n",
    "    out_path = f\"resnet_qat_qdq_b{bs}_op18.onnx\"\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy,\n",
    "        out_path,\n",
    "        opset_version=18,          # ✅ REQUIRED for Q/DQ\n",
    "        do_constant_folding=False, # ✅ MUST be False for QAT\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes=None,         # ✅ STATIC shapes\n",
    "    )\n",
    "\n",
    "    print(\"Exported:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd387c9-11eb-4c8e-a414-0a0dd9300f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 84K Dec 14 11:53 resnet_qat_qdq_b1_op18.onnx\n",
      "-rw-r--r-- 1 ihsiao ihsiao 84K Dec 14 11:53 resnet_qat_qdq_b64_op18.onnx\n",
      "-rw-r--r-- 1 ihsiao ihsiao 84K Dec 14 11:53 resnet_qat_qdq_b128_op18.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -lh resnet_qat_qdq_b1_op18.onnx\n",
    "!ls -lh resnet_qat_qdq_b64_op18.onnx\n",
    "!ls -lh resnet_qat_qdq_b128_op18.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4782b1d-c490-4e17-9a75-575c3e84becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2025-12:16:02] [TRT] [W] WARNING The logger passed into createInferBuilder differs from one already registered for an existing builder, runtime, or refitter. So the current new logger is ignored, and TensorRT will use the existing one which is returned by nvinfer1::getLogger() instead.\n",
      "[12/14/2025-12:16:02] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-12:16:02] [TRT] [I] ONNX IR version:  0.0.10\n",
      "[12/14/2025-12:16:02] [TRT] [I] Opset version:    18\n",
      "[12/14/2025-12:16:02] [TRT] [I] Producer name:    pytorch\n",
      "[12/14/2025-12:16:02] [TRT] [I] Producer version: 2.9.1+cu128\n",
      "[12/14/2025-12:16:02] [TRT] [I] Domain:           \n",
      "[12/14/2025-12:16:02] [TRT] [I] Model version:    0\n",
      "[12/14/2025-12:16:02] [TRT] [I] Doc string:       \n",
      "[12/14/2025-12:16:02] [TRT] [I] ----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_5287623/ipykernel_2251225/4082687543.py:75: DeprecationWarning: Use Deprecated in TensorRT 10.12. Superseded by strong typing. instead.\n",
      "  network.get_output(i).dtype = trt.float32\n",
      "/tmp/SLURM_5287623/ipykernel_2251225/4082687543.py:87: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.\n",
      "  config.int8_calibrator = EntropyCalibrator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2025-12:16:03] [TRT] [I] Perform graph optimization on calibration graph.\n",
      "[12/14/2025-12:16:03] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-12:16:03] [TRT] [I] Compiler backend is used during engine build.\n",
      "[12/14/2025-12:16:04] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-12:16:05] [TRT] [I] Total Host Persistent Memory: 148544 bytes\n",
      "[12/14/2025-12:16:05] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/14/2025-12:16:05] [TRT] [I] Max Scratch Memory: 4608 bytes\n",
      "[12/14/2025-12:16:05] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 102 steps to complete.\n",
      "[12/14/2025-12:16:05] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.596822ms to assign 4 blocks to 102 nodes requiring 201216 bytes.\n",
      "[12/14/2025-12:16:05] [TRT] [I] Total Activation Memory: 201216 bytes\n",
      "[12/14/2025-12:16:05] [TRT] [I] Total Weights Memory: 3251752 bytes\n",
      "[12/14/2025-12:16:05] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[12/14/2025-12:16:05] [TRT] [I] Engine generation completed in 1.48832 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 4 (MiB)\n",
      "[12/14/2025-12:16:05] [TRT] [I] Starting Calibration.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 0 in 0.00963719 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 1 in 0.00735308 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 2 in 0.00685557 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 3 in 0.00681813 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 4 in 0.00686406 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 5 in 0.00684717 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 6 in 0.00682171 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 7 in 0.00673818 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 8 in 0.00676742 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 9 in 0.00675564 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 10 in 0.00676968 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 11 in 0.00678602 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 12 in 0.00678536 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 13 in 0.00674745 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 14 in 0.00672681 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 15 in 0.00701125 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 16 in 0.00689272 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 17 in 0.00684545 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 18 in 0.00685567 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 19 in 0.00689954 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 20 in 0.00682584 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 21 in 0.00687661 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 22 in 0.00686062 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 23 in 0.00687771 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 24 in 0.00703889 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 25 in 0.00676314 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 26 in 0.00677706 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 27 in 0.00673429 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 28 in 0.00679095 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 29 in 0.00675922 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 30 in 0.0067827 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 31 in 0.0067743 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 32 in 0.00675334 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 33 in 0.00684468 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 34 in 0.00671614 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 35 in 0.00671928 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 36 in 0.00679006 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 37 in 0.00674036 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 38 in 0.00696046 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 39 in 0.00690206 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 40 in 0.00682962 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 41 in 0.00688903 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 42 in 0.00687744 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 43 in 0.00687644 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 44 in 0.00689629 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 45 in 0.00692842 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 46 in 0.0068594 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 47 in 0.00686789 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 48 in 0.00689293 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 49 in 0.00688392 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 50 in 0.00685013 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 51 in 0.00685278 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 52 in 0.00684478 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 53 in 0.00691946 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 54 in 0.00690639 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 55 in 0.00690721 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 56 in 0.00690734 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 57 in 0.00686641 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 58 in 0.00689708 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 59 in 0.00691884 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 60 in 0.006885 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 61 in 0.00688561 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 62 in 0.00689919 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 63 in 0.00686492 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 64 in 0.00687126 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 65 in 0.00690837 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 66 in 0.00684599 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 67 in 0.00688714 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 68 in 0.00687371 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 69 in 0.00686723 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 70 in 0.00688756 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 71 in 0.00695018 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 72 in 0.00675868 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 73 in 0.00679077 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 74 in 0.00674231 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 75 in 0.00675971 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 76 in 0.00675332 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 77 in 0.00673531 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 78 in 0.0067473 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 79 in 0.00680206 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 80 in 0.00679459 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 81 in 0.00676337 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 82 in 0.00677278 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 83 in 0.00670887 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 84 in 0.00676751 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 85 in 0.00702322 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 86 in 0.00693847 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 87 in 0.00678423 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 88 in 0.00676974 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 89 in 0.00680421 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 90 in 0.00681811 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 91 in 0.00674534 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 92 in 0.00676938 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 93 in 0.00679758 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 94 in 0.00676207 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 95 in 0.00676834 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 96 in 0.00676894 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 97 in 0.00679307 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 98 in 0.00675563 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 99 in 0.00676867 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 100 in 0.00698572 seconds.\n",
      "[12/14/2025-12:16:05] [TRT] [I]   Calibrated batch 101 in 0.00689959 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 102 in 0.0069194 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 103 in 0.00684717 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 104 in 0.00685745 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 105 in 0.00690019 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 106 in 0.00684419 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 107 in 0.00687284 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 108 in 0.00686816 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 109 in 0.00683347 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 110 in 0.00943635 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 111 in 0.00696336 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 112 in 0.00687519 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 113 in 0.00687685 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 114 in 0.00684569 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 115 in 0.00687232 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 116 in 0.0069194 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 117 in 0.00684392 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 118 in 0.00684707 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 119 in 0.00689874 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 120 in 0.00684182 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 121 in 0.00686239 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 122 in 0.00688836 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 123 in 0.00688356 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 124 in 0.00687322 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 125 in 0.00686529 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 126 in 0.00685258 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 127 in 0.00690657 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 128 in 0.00688144 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 129 in 0.00682629 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 130 in 0.00690939 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 131 in 0.00681786 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 132 in 0.00685852 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 133 in 0.00690581 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 134 in 0.00682758 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 135 in 0.00685834 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 136 in 0.00686335 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 137 in 0.00683211 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 138 in 0.00688478 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 139 in 0.00690321 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 140 in 0.00685652 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 141 in 0.00691208 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 142 in 0.00696811 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 143 in 0.00673076 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 144 in 0.00678662 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 145 in 0.00676756 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 146 in 0.00680034 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 147 in 0.00675503 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 148 in 0.0067495 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 149 in 0.00673308 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 150 in 0.00673172 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 151 in 0.00674719 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 152 in 0.00676607 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 153 in 0.0067395 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 154 in 0.00674366 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 155 in 0.00674643 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 156 in 0.00702178 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 157 in 0.00689422 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 158 in 0.00687387 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 159 in 0.00689417 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 160 in 0.00686557 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 161 in 0.00682525 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 162 in 0.0068594 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 163 in 0.00685664 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 164 in 0.00687315 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 165 in 0.00685339 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 166 in 0.00686638 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 167 in 0.00684141 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 168 in 0.00685826 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 169 in 0.00685234 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 170 in 0.00685398 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 171 in 0.00684739 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 172 in 0.00686378 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 173 in 0.00693791 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 174 in 0.00684758 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 175 in 0.00686783 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 176 in 0.00690905 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 177 in 0.00688211 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 178 in 0.00687877 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 179 in 0.00692027 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 180 in 0.00689448 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 181 in 0.00707833 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 182 in 0.00679332 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 183 in 0.00675935 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 184 in 0.00677684 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 185 in 0.00680366 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 186 in 0.00681935 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 187 in 0.0068388 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 188 in 0.00677115 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 189 in 0.00678426 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 190 in 0.00674729 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 191 in 0.00676197 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 192 in 0.00677226 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 193 in 0.00678905 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 194 in 0.00675604 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 195 in 0.00676268 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 196 in 0.00707887 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 197 in 0.00687859 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 198 in 0.00685751 seconds.\n",
      "[12/14/2025-12:16:06] [TRT] [I]   Calibrated batch 199 in 0.00689879 seconds.\n",
      "[12/14/2025-12:16:12] [TRT] [I]   Post Processing Calibration data in 5.8659 seconds.\n",
      "[12/14/2025-12:16:12] [TRT] [I] Calibration completed in 9.00527 seconds.\n",
      "[12/14/2025-12:16:12] [TRT] [I] Writing Calibration Cache for calibrator: TRT-101401-EntropyCalibration2\n",
      "[12/14/2025-12:16:12] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-12:16:50] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-12:16:51] [TRT] [I] Total Host Persistent Memory: 171584 bytes\n",
      "[12/14/2025-12:16:51] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/14/2025-12:16:51] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/14/2025-12:16:51] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 36 steps to complete.\n",
      "[12/14/2025-12:16:51] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.119805ms to assign 3 blocks to 36 nodes requiring 98304 bytes.\n",
      "[12/14/2025-12:16:51] [TRT] [I] Total Activation Memory: 98304 bytes\n",
      "[12/14/2025-12:16:51] [TRT] [I] Total Weights Memory: 605188 bytes\n",
      "[12/14/2025-12:16:51] [TRT] [I] Engine generation completed in 38.5956 seconds.\n",
      "[12/14/2025-12:16:51] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 41 MiB\n",
      "Saved: resnet_ptq_int8_b1.engine\n",
      "[12/14/2025-12:16:51] [TRT] [W] WARNING The logger passed into createInferBuilder differs from one already registered for an existing builder, runtime, or refitter. So the current new logger is ignored, and TensorRT will use the existing one which is returned by nvinfer1::getLogger() instead.\n",
      "[12/14/2025-12:16:51] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-12:16:51] [TRT] [I] ONNX IR version:  0.0.10\n",
      "[12/14/2025-12:16:51] [TRT] [I] Opset version:    18\n",
      "[12/14/2025-12:16:51] [TRT] [I] Producer name:    pytorch\n",
      "[12/14/2025-12:16:51] [TRT] [I] Producer version: 2.9.1+cu128\n",
      "[12/14/2025-12:16:51] [TRT] [I] Domain:           \n",
      "[12/14/2025-12:16:51] [TRT] [I] Model version:    0\n",
      "[12/14/2025-12:16:51] [TRT] [I] Doc string:       \n",
      "[12/14/2025-12:16:51] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-12:16:52] [TRT] [I] Perform graph optimization on calibration graph.\n",
      "[12/14/2025-12:16:52] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-12:16:52] [TRT] [I] Compiler backend is used during engine build.\n",
      "[12/14/2025-12:16:54] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-12:16:55] [TRT] [I] Total Host Persistent Memory: 113280 bytes\n",
      "[12/14/2025-12:16:55] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/14/2025-12:16:55] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/14/2025-12:16:55] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 83 steps to complete.\n",
      "[12/14/2025-12:16:55] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.340319ms to assign 3 blocks to 83 nodes requiring 12582912 bytes.\n",
      "[12/14/2025-12:16:55] [TRT] [I] Total Activation Memory: 12582912 bytes\n",
      "[12/14/2025-12:16:55] [TRT] [I] Total Weights Memory: 3415592 bytes\n",
      "[12/14/2025-12:16:55] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[12/14/2025-12:16:55] [TRT] [I] Engine generation completed in 2.54345 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +12, now: CPU 0, GPU 16 (MiB)\n",
      "[12/14/2025-12:16:55] [TRT] [I] Starting Calibration.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 0 in 0.0143547 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 1 in 0.0116269 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 2 in 0.0115697 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 3 in 0.0115313 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 4 in 0.0115142 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 5 in 0.0116755 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 6 in 0.011637 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 7 in 0.0116639 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 8 in 0.0115981 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 9 in 0.0118102 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 10 in 0.0114846 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 11 in 0.0115352 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 12 in 0.0116019 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 13 in 0.0116118 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 14 in 0.0117909 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 15 in 0.0108376 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 16 in 0.0108167 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 17 in 0.0108191 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 18 in 0.0107798 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 19 in 0.0108425 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 20 in 0.0107908 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 21 in 0.0108266 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 22 in 0.0108503 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 23 in 0.0112985 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 24 in 0.0116681 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 25 in 0.0115685 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 26 in 0.0115237 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 27 in 0.0115002 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 28 in 0.0120821 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 29 in 0.012255 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 30 in 0.0119262 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 31 in 0.0119079 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 32 in 0.0118281 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 33 in 0.0116933 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 34 in 0.0115521 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 35 in 0.0117346 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 36 in 0.0118048 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 37 in 0.0117554 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 38 in 0.0116876 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 39 in 0.0117952 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 40 in 0.0117648 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 41 in 0.0118373 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 42 in 0.0117496 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 43 in 0.0117557 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 44 in 0.0117048 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 45 in 0.0116171 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 46 in 0.0116778 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 47 in 0.0116497 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 48 in 0.0115628 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 49 in 0.0115683 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 50 in 0.0118113 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 51 in 0.0117421 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 52 in 0.011702 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 53 in 0.0116585 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 54 in 0.0116394 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 55 in 0.0116483 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 56 in 0.0116511 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 57 in 0.0115897 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 58 in 0.0116014 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 59 in 0.0116158 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 60 in 0.0116001 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 61 in 0.0115108 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 62 in 0.0116899 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 63 in 0.0115528 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 64 in 0.011592 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 65 in 0.0116155 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 66 in 0.0117131 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 67 in 0.0116185 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 68 in 0.0121195 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 69 in 0.011699 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 70 in 0.0116638 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 71 in 0.0116193 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 72 in 0.0116063 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 73 in 0.0116396 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 74 in 0.0118041 seconds.\n",
      "[12/14/2025-12:16:55] [TRT] [I]   Calibrated batch 75 in 0.0115523 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 76 in 0.0116714 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 77 in 0.0116412 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 78 in 0.011684 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 79 in 0.0116722 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 80 in 0.0115912 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 81 in 0.0116932 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 82 in 0.0115643 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 83 in 0.011674 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 84 in 0.0117102 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 85 in 0.0119579 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 86 in 0.0116354 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 87 in 0.011574 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 88 in 0.0115857 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 89 in 0.0115521 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 90 in 0.0116581 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 91 in 0.0116182 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 92 in 0.0115583 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 93 in 0.011989 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 94 in 0.0115428 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 95 in 0.0115141 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 96 in 0.0115628 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 97 in 0.0115801 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 98 in 0.0116188 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 99 in 0.0115044 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 100 in 0.011571 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 101 in 0.0115259 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 102 in 0.0119225 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 103 in 0.0115598 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 104 in 0.0115245 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 105 in 0.0115485 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 106 in 0.0115047 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 107 in 0.0115357 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 108 in 0.0115711 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 109 in 0.0115942 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 110 in 0.0115528 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 111 in 0.0119278 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 112 in 0.0115211 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 113 in 0.0115706 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 114 in 0.0115421 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 115 in 0.0115766 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 116 in 0.0115984 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 117 in 0.0116085 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 118 in 0.011471 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 119 in 0.0116002 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 120 in 0.0120565 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 121 in 0.0116374 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 122 in 0.0116466 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 123 in 0.0116209 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 124 in 0.0116493 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 125 in 0.0117248 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 126 in 0.011753 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 127 in 0.0117904 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 128 in 0.0117878 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 129 in 0.0117378 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 130 in 0.0117158 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 131 in 0.0119978 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 132 in 0.0116376 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 133 in 0.0116612 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 134 in 0.0116847 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 135 in 0.0115594 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 136 in 0.0115166 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 137 in 0.0115404 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 138 in 0.0116056 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 139 in 0.0115022 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 140 in 0.0119645 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 141 in 0.011592 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 142 in 0.0115092 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 143 in 0.0115501 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 144 in 0.0115194 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 145 in 0.0115642 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 146 in 0.0116002 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 147 in 0.0115285 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 148 in 0.0115033 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 149 in 0.0118414 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 150 in 0.0116645 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 151 in 0.011557 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 152 in 0.0115225 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 153 in 0.0115187 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 154 in 0.0115285 seconds.\n",
      "[12/14/2025-12:16:56] [TRT] [I]   Calibrated batch 155 in 0.0116254 seconds.\n",
      "[12/14/2025-12:17:02] [TRT] [I]   Post Processing Calibration data in 5.69513 seconds.\n",
      "[12/14/2025-12:17:02] [TRT] [I] Calibration completed in 10.1822 seconds.\n",
      "[12/14/2025-12:17:02] [TRT] [I] Writing Calibration Cache for calibrator: TRT-101401-EntropyCalibration2\n",
      "[12/14/2025-12:17:02] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-12:17:37] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-12:17:38] [TRT] [I] Total Host Persistent Memory: 176640 bytes\n",
      "[12/14/2025-12:17:38] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/14/2025-12:17:38] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/14/2025-12:17:38] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 37 steps to complete.\n",
      "[12/14/2025-12:17:38] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.117321ms to assign 3 blocks to 37 nodes requiring 6291456 bytes.\n",
      "[12/14/2025-12:17:38] [TRT] [I] Total Activation Memory: 6291456 bytes\n",
      "[12/14/2025-12:17:38] [TRT] [I] Total Weights Memory: 553988 bytes\n",
      "[12/14/2025-12:17:38] [TRT] [I] Engine generation completed in 36.2393 seconds.\n",
      "[12/14/2025-12:17:38] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 41 MiB\n",
      "Saved: resnet_ptq_int8_b64.engine\n",
      "[12/14/2025-12:17:39] [TRT] [W] WARNING The logger passed into createInferBuilder differs from one already registered for an existing builder, runtime, or refitter. So the current new logger is ignored, and TensorRT will use the existing one which is returned by nvinfer1::getLogger() instead.\n",
      "[12/14/2025-12:17:39] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-12:17:39] [TRT] [I] ONNX IR version:  0.0.10\n",
      "[12/14/2025-12:17:39] [TRT] [I] Opset version:    18\n",
      "[12/14/2025-12:17:39] [TRT] [I] Producer name:    pytorch\n",
      "[12/14/2025-12:17:39] [TRT] [I] Producer version: 2.9.1+cu128\n",
      "[12/14/2025-12:17:39] [TRT] [I] Domain:           \n",
      "[12/14/2025-12:17:39] [TRT] [I] Model version:    0\n",
      "[12/14/2025-12:17:39] [TRT] [I] Doc string:       \n",
      "[12/14/2025-12:17:39] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-12:17:39] [TRT] [I] Perform graph optimization on calibration graph.\n",
      "[12/14/2025-12:17:39] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-12:17:39] [TRT] [I] Compiler backend is used during engine build.\n",
      "[12/14/2025-12:17:41] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-12:17:41] [TRT] [I] Total Host Persistent Memory: 113280 bytes\n",
      "[12/14/2025-12:17:41] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/14/2025-12:17:41] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/14/2025-12:17:41] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 83 steps to complete.\n",
      "[12/14/2025-12:17:41] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.363513ms to assign 3 blocks to 83 nodes requiring 25165824 bytes.\n",
      "[12/14/2025-12:17:41] [TRT] [I] Total Activation Memory: 25165824 bytes\n",
      "[12/14/2025-12:17:42] [TRT] [I] Total Weights Memory: 3415592 bytes\n",
      "[12/14/2025-12:17:42] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[12/14/2025-12:17:42] [TRT] [I] Engine generation completed in 2.24182 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +24, now: CPU 0, GPU 28 (MiB)\n",
      "[12/14/2025-12:17:42] [TRT] [I] Starting Calibration.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 0 in 0.019033 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 1 in 0.0166559 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 2 in 0.0165578 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 3 in 0.0166091 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 4 in 0.01652 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 5 in 0.0163818 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 6 in 0.01633 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 7 in 0.0163992 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 8 in 0.0163416 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 9 in 0.0165727 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 10 in 0.0165253 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 11 in 0.0165068 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 12 in 0.0163519 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 13 in 0.0164292 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 14 in 0.0164035 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 15 in 0.0163424 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 16 in 0.016338 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 17 in 0.0164419 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 18 in 0.0164175 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 19 in 0.0164708 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 20 in 0.0164784 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 21 in 0.0162694 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 22 in 0.0163765 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 23 in 0.0163803 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 24 in 0.0163699 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 25 in 0.0164437 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 26 in 0.0166671 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 27 in 0.0164395 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 28 in 0.0164077 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 29 in 0.0164248 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 30 in 0.0164127 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 31 in 0.0164538 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 32 in 0.0163268 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 33 in 0.0163233 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 34 in 0.0163991 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 35 in 0.0163451 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 36 in 0.0163528 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 37 in 0.0163956 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 38 in 0.0166982 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 39 in 0.0164998 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 40 in 0.0164201 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 41 in 0.0164666 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 42 in 0.0165094 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 43 in 0.0165153 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 44 in 0.0165384 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 45 in 0.0163323 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 46 in 0.0163024 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 47 in 0.0163791 seconds.\n",
      "[12/14/2025-12:17:42] [TRT] [I]   Calibrated batch 48 in 0.0163799 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 49 in 0.0164227 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 50 in 0.0167042 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 51 in 0.0164096 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 52 in 0.0163314 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 53 in 0.0164283 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 54 in 0.0163747 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 55 in 0.0164228 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 56 in 0.0165551 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 57 in 0.0163166 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 58 in 0.0163419 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 59 in 0.0163375 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 60 in 0.0164113 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 61 in 0.0164362 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 62 in 0.0165624 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 63 in 0.0164655 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 64 in 0.0164068 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 65 in 0.0164565 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 66 in 0.0164435 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 67 in 0.0165118 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 68 in 0.0165261 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 69 in 0.016316 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 70 in 0.0163028 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 71 in 0.0163792 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 72 in 0.0163009 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 73 in 0.0163927 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 74 in 0.0165701 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 75 in 0.0163852 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 76 in 0.0163528 seconds.\n",
      "[12/14/2025-12:17:43] [TRT] [I]   Calibrated batch 77 in 0.016309 seconds.\n",
      "[12/14/2025-12:17:49] [TRT] [I]   Post Processing Calibration data in 5.64959 seconds.\n",
      "[12/14/2025-12:17:49] [TRT] [I] Calibration completed in 9.33537 seconds.\n",
      "[12/14/2025-12:17:49] [TRT] [I] Writing Calibration Cache for calibrator: TRT-101401-EntropyCalibration2\n",
      "[12/14/2025-12:17:49] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-12:18:22] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-12:18:23] [TRT] [I] Total Host Persistent Memory: 184704 bytes\n",
      "[12/14/2025-12:18:23] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/14/2025-12:18:23] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/14/2025-12:18:23] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 37 steps to complete.\n",
      "[12/14/2025-12:18:23] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.150002ms to assign 3 blocks to 37 nodes requiring 12582912 bytes.\n",
      "[12/14/2025-12:18:23] [TRT] [I] Total Activation Memory: 12582912 bytes\n",
      "[12/14/2025-12:18:23] [TRT] [I] Total Weights Memory: 560132 bytes\n",
      "[12/14/2025-12:18:23] [TRT] [I] Engine generation completed in 34.7398 seconds.\n",
      "[12/14/2025-12:18:23] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 41 MiB\n",
      "Saved: resnet_ptq_int8_b128.engine\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "\n",
    "onnx_map = {\n",
    "    1:   \"resnet_int8_b1_op18.onnx\",\n",
    "    64:  \"resnet_int8_b64_op18.onnx\",\n",
    "    128: \"resnet_int8_b128_op18.onnx\",\n",
    "}\n",
    "\n",
    "calib_loader_map = {\n",
    "    1:   test_loader_b1,\n",
    "    64:  test_loader_b64,\n",
    "    128: test_loader_b128,\n",
    "}\n",
    "\n",
    "class EntropyCalibrator(trt.IInt8EntropyCalibrator2):\n",
    "    def __init__(self, calib_loader, max_batches=200, cache_file=\"calib.cache\"):\n",
    "        super().__init__()\n",
    "        self.cache_file = cache_file\n",
    "        self.data_iter = iter(calib_loader)\n",
    "        self.max_batches = max_batches\n",
    "        self.batch_count = 0\n",
    "        x0, _ = next(iter(calib_loader))\n",
    "        self.batch_size = x0.shape[0]\n",
    "        self.device_input = torch.empty_like(x0, device=\"cuda\")\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def get_batch(self, names):\n",
    "        if self.batch_count >= self.max_batches:\n",
    "            return None\n",
    "        try:\n",
    "            x, _ = next(self.data_iter)\n",
    "        except StopIteration:\n",
    "            return None\n",
    "        self.batch_count += 1\n",
    "        x = x.to(\"cuda\", non_blocking=True)\n",
    "        self.device_input.resize_(x.shape).copy_(x)\n",
    "        return [int(self.device_input.data_ptr())]\n",
    "\n",
    "    def read_calibration_cache(self):\n",
    "        if os.path.exists(self.cache_file):\n",
    "            with open(self.cache_file, \"rb\") as f:\n",
    "                return f.read()\n",
    "        return None\n",
    "\n",
    "    def write_calibration_cache(self, cache):\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            f.write(cache)\n",
    "\n",
    "def build_int8_engine_static(onnx_path, engine_path, calib_loader, max_calib_batches=200):\n",
    "    cache_path = engine_path.replace(\".engine\", \".cache\")\n",
    "\n",
    "    # ✅ CRITICAL: delete stale cache (common cause of ~10–25% accuracy)\n",
    "    if os.path.exists(cache_path):\n",
    "        os.remove(cache_path)\n",
    "        print(\"Deleted stale cache:\", cache_path)\n",
    "\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "        with open(onnx_path, \"rb\") as f:\n",
    "            if not parser.parse(f.read()):\n",
    "                for i in range(parser.num_errors):\n",
    "                    print(parser.get_error(i))\n",
    "                raise RuntimeError(f\"ONNX parse failed: {onnx_path}\")\n",
    "\n",
    "        # ✅ Force network output to FP32 (prevents INT8 logits / clipping issues)\n",
    "        for i in range(network.num_outputs):\n",
    "            network.get_output(i).dtype = trt.float32\n",
    "\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)\n",
    "\n",
    "        config.set_flag(trt.BuilderFlag.INT8)\n",
    "        config.set_flag(trt.BuilderFlag.FP16)  # safe + helps performance/fallback\n",
    "\n",
    "        # Helps TRT respect dtype constraints (esp. keeping outputs FP32)\n",
    "        if hasattr(trt.BuilderFlag, \"OBEY_PRECISION_CONSTRAINTS\"):\n",
    "            config.set_flag(trt.BuilderFlag.OBEY_PRECISION_CONSTRAINTS)\n",
    "\n",
    "        config.int8_calibrator = EntropyCalibrator(\n",
    "            calib_loader,\n",
    "            max_batches=max_calib_batches,\n",
    "            cache_file=cache_path\n",
    "        )\n",
    "\n",
    "        serialized = builder.build_serialized_network(network, config)\n",
    "        if serialized is None:\n",
    "            raise RuntimeError(f\"Engine build failed: {onnx_path}\")\n",
    "\n",
    "        with open(engine_path, \"wb\") as f:\n",
    "            f.write(serialized)\n",
    "\n",
    "    print(f\"Saved: {engine_path}\")\n",
    "\n",
    "# build all three\n",
    "for bs, onnx_path in onnx_map.items():\n",
    "    build_int8_engine_static(\n",
    "        onnx_path=onnx_path,\n",
    "        engine_path=f\"resnet_ptq_int8_b{bs}.engine\",\n",
    "        calib_loader=calib_loader_map[bs],\n",
    "        max_calib_batches=200,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f3b6a1e-5d5a-46e8-817c-3029bd41ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "@torch.no_grad()\n",
    "def trt_accuracy_static(engine_path, test_loader, num_batches=None):\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "    with open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
    "    inp = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.INPUT][0]\n",
    "    out = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.OUTPUT][0]\n",
    "\n",
    "    # engine fixed shapes\n",
    "    in_shape = tuple(engine.get_tensor_shape(inp))\n",
    "    out_shape = tuple(engine.get_tensor_shape(out))\n",
    "    fixed_bsz = in_shape[0]  # should be 1 or 64 or 128\n",
    "\n",
    "    # output dtype\n",
    "    trt_dtype = engine.get_tensor_dtype(out)\n",
    "    torch_dtype = {\n",
    "        trt.DataType.FLOAT: torch.float32,\n",
    "        trt.DataType.HALF:  torch.float16,\n",
    "        trt.DataType.INT8:  torch.int8,\n",
    "        trt.DataType.INT32: torch.int32,\n",
    "    }[trt_dtype]\n",
    "\n",
    "    stream = torch.cuda.current_stream()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for bi, (x_cpu, y_cpu) in enumerate(test_loader):\n",
    "        if num_batches is not None and bi >= num_batches:\n",
    "            break\n",
    "\n",
    "        x = x_cpu.to(\"cuda\", non_blocking=True)\n",
    "        y = y_cpu.to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        if x.shape[0] != fixed_bsz:\n",
    "            raise RuntimeError(f\"Batch mismatch: loader={x.shape[0]} but engine expects {fixed_bsz}\")\n",
    "\n",
    "        yhat = torch.empty(out_shape, device=\"cuda\", dtype=torch_dtype)\n",
    "\n",
    "        context.set_tensor_address(inp, int(x.data_ptr()))\n",
    "        context.set_tensor_address(out, int(yhat.data_ptr()))\n",
    "\n",
    "        ok = context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "        if not ok:\n",
    "            raise RuntimeError(\"TRT execute failed\")\n",
    "\n",
    "        pred = yhat.float().argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.shape[0]\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8e79f72-df0b-43c7-864f-2c95526aa779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2025-12:19:33] [TRT] [W] WARNING The logger passed into createInferRuntime differs from one already registered for an existing builder, runtime, or refitter. So the current new logger is ignored, and TensorRT will use the existing one which is returned by nvinfer1::getLogger() instead.\n",
      "[12/14/2025-12:19:33] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/14/2025-12:19:41] [TRT] [W] WARNING The logger passed into createInferRuntime differs from one already registered for an existing builder, runtime, or refitter. So the current new logger is ignored, and TensorRT will use the existing one which is returned by nvinfer1::getLogger() instead.\n",
      "[12/14/2025-12:19:41] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/14/2025-12:19:42] [TRT] [W] WARNING The logger passed into createInferRuntime differs from one already registered for an existing builder, runtime, or refitter. So the current new logger is ignored, and TensorRT will use the existing one which is returned by nvinfer1::getLogger() instead.\n",
      "[12/14/2025-12:19:42] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "INT8 TRT Acc b1:   22.36%\n",
      "INT8 TRT Acc b64:  22.25%\n",
      "INT8 TRT Acc b128: 22.30%\n"
     ]
    }
   ],
   "source": [
    "acc1   = trt_accuracy_static(\"resnet_ptq_int8_b1.engine\",   test_loader_b1)\n",
    "acc64  = trt_accuracy_static(\"resnet_ptq_int8_b64.engine\",  test_loader_b64)\n",
    "acc128 = trt_accuracy_static(\"resnet_ptq_int8_b128.engine\", test_loader_b128)\n",
    "\n",
    "print(f\"INT8 TRT Acc b1:   {acc1:.2f}%\")\n",
    "print(f\"INT8 TRT Acc b64:  {acc64:.2f}%\")\n",
    "print(f\"INT8 TRT Acc b128: {acc128:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7920f64d-8066-414b-b225-3ac2f5ff2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "m = onnx.load(out_onnx)\n",
    "ops = set(n.op_type for n in m.graph.node)\n",
    "print(\"QuantizeLinear:\", \"QuantizeLinear\" in ops)\n",
    "print(\"DequantizeLinear:\", \"DequantizeLinear\" in ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af714ce-1796-4369-91c5-e01eeafedcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = torch.load(\"../../pth/resnet_qat_preconvert.pth\", map_location=\"cpu\")\n",
    "print(\"num keys:\", len(ckpt))\n",
    "print(\"first 40 keys:\")\n",
    "for k in list(ckpt.keys())[:40]:\n",
    "    print(\" \", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc83341-2d32-4830-988e-6cea09e18194",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing, unexpected = model.load_state_dict(ckpt, strict=False)\n",
    "print(\"missing:\", len(missing))\n",
    "print(\"unexpected:\", len(unexpected))\n",
    "print(\"example missing:\", missing[:20])\n",
    "print(\"example unexpected:\", unexpected[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7170064d-9517-4379-9e55-20139390dc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ops: ['Add', 'Conv', 'Gemm', 'ReduceMean', 'Relu', 'Reshape']\n",
      "Has QuantizeLinear: False\n",
      "Has DequantizeLinear: False\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "m = onnx.load(\"resnet_int8_b1_op18.onnx\")\n",
    "ops = sorted({n.op_type for n in m.graph.node})\n",
    "\n",
    "print(\"Ops:\", ops)\n",
    "print(\"Has QuantizeLinear:\", \"QuantizeLinear\" in ops)\n",
    "print(\"Has DequantizeLinear:\", \"DequantizeLinear\" in ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5118bd-7eb8-4498-a680-62ab16a7fa3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee599 (PyTorch)",
   "language": "python",
   "name": "ee599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
