{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9fb18d4-66d7-421d-bf00-63460b5c60e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.9.1+cu128 CUDA: True\n",
      "TensorRT: 10.14.1.48.post1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())\n",
    "print(\"TensorRT:\", trt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e20c95b9-71ae-476f-9fe5-3997105ecf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parents[1]))\n",
    "\n",
    "from models.resnet32_model import ResNet, ResNetQAT\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd93386-e7a2-4028-8377-27899c33347a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetQAT(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlockQAT(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlockQAT(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNetQAT(num_classes=10)\n",
    "\n",
    "state = torch.load(\"../../pth/resnet_qat_preconvert.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(state, strict=False)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4ea8db1-83f3-4806-8401-14ac8fcedd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_5287623/ipykernel_2234128/2646228436.py:6: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model_int8 = tq.convert(model, inplace=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`... âœ…\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... âœ…\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... âœ…\n",
      "Applied 99 of general pattern rewrite rules.\n",
      "Exported resnet_qat_int8_b1_op18.onnx\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`... âœ…\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... âœ…\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... âœ…\n",
      "Applied 99 of general pattern rewrite rules.\n",
      "Exported resnet_qat_int8_b64_op18.onnx\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ResNetQAT([...]` with `torch.export.export(..., strict=False)`... âœ…\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... âœ…\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... âœ…\n",
      "Applied 99 of general pattern rewrite rules.\n",
      "Exported resnet_qat_int8_b128_op18.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch.ao.quantization as tq\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# ðŸ”‘ CRITICAL STEP\n",
    "model_int8 = tq.convert(model, inplace=False)\n",
    "\n",
    "dummy_map = {\n",
    "    1:   torch.randn(1,   3, 32, 32),\n",
    "    64:  torch.randn(64,  3, 32, 32),\n",
    "    128: torch.randn(128, 3, 32, 32),\n",
    "}\n",
    "\n",
    "for bs, dummy in dummy_map.items():\n",
    "    out_path = f\"resnet_qat_int8_b{bs}_op18.onnx\"\n",
    "    torch.onnx.export(\n",
    "        model_int8, dummy, out_path,\n",
    "        opset_version=18,\n",
    "        do_constant_folding=False,   # IMPORTANT for QAT\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes=None,\n",
    "    )\n",
    "    print(\"Exported\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "823f10ed-5f1e-47fb-bb67-43f59872e074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantizeLinear: False\n",
      "DequantizeLinear: False\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "m = onnx.load(\"resnet_qat_int8_b1_op18.onnx\")\n",
    "ops = sorted(set(n.op_type for n in m.graph.node))\n",
    "print(\"QuantizeLinear:\", \"QuantizeLinear\" in ops)\n",
    "print(\"DequantizeLinear:\", \"DequantizeLinear\" in ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17143253-186e-4c27-ad4a-c67f1e4d6429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 78K Dec 14 09:47 resnet_qat_int8_b1_op18.onnx\n",
      "-rw-r--r-- 1 ihsiao ihsiao 78K Dec 14 09:47 resnet_qat_int8_b64_op18.onnx\n",
      "-rw-r--r-- 1 ihsiao ihsiao 78K Dec 14 09:47 resnet_qat_int8_b128_op18.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -lh resnet_qat_int8_b1_op18.onnx\n",
    "!ls -lh resnet_qat_int8_b64_op18.onnx\n",
    "!ls -lh resnet_qat_int8_b128_op18.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b85a783e-e0c7-4e28-ae74-93c7507e8996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 18)]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "m = onnx.load(\"resnet_qat_int8_b1_op13.onnx\")\n",
    "onnx.checker.check_model(m)\n",
    "print([(op.domain, op.version) for op in m.opset_import])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5ac89d5-edbc-4178-b9e1-8f408f840f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.14.1.48.post1\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "print(trt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "250f0dd1-9496-4b88-add3-6ab7503cd722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53f3b54e-aac2-4547-af94-14fdf8d9f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader_b1   = DataLoader(test_dataset, batch_size=1,   shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\n",
    "test_loader_b64  = DataLoader(test_dataset, batch_size=64,  shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\n",
    "test_loader_b128 = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ed5192c-3830-4e49-a652-573ceaa48587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "@torch.no_grad()\n",
    "def trt_accuracy_static(engine_path, test_loader, num_batches=None):\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "    with open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
    "    inp = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.INPUT][0]\n",
    "    out = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.OUTPUT][0]\n",
    "\n",
    "    # engine fixed shapes\n",
    "    in_shape = tuple(engine.get_tensor_shape(inp))\n",
    "    out_shape = tuple(engine.get_tensor_shape(out))\n",
    "    fixed_bsz = in_shape[0]  # should be 1 or 64 or 128\n",
    "\n",
    "    # output dtype\n",
    "    trt_dtype = engine.get_tensor_dtype(out)\n",
    "    torch_dtype = {\n",
    "        trt.DataType.FLOAT: torch.float32,\n",
    "        trt.DataType.HALF:  torch.float16,\n",
    "        trt.DataType.INT8:  torch.int8,\n",
    "        trt.DataType.INT32: torch.int32,\n",
    "    }[trt_dtype]\n",
    "\n",
    "    stream = torch.cuda.current_stream()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for bi, (x_cpu, y_cpu) in enumerate(test_loader):\n",
    "        if num_batches is not None and bi >= num_batches:\n",
    "            break\n",
    "\n",
    "        x = x_cpu.to(\"cuda\", non_blocking=True)\n",
    "        y = y_cpu.to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        if x.shape[0] != fixed_bsz:\n",
    "            raise RuntimeError(f\"Batch mismatch: loader={x.shape[0]} but engine expects {fixed_bsz}\")\n",
    "\n",
    "        yhat = torch.empty(out_shape, device=\"cuda\", dtype=torch_dtype)\n",
    "\n",
    "        context.set_tensor_address(inp, int(x.data_ptr()))\n",
    "        context.set_tensor_address(out, int(yhat.data_ptr()))\n",
    "\n",
    "        ok = context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "        if not ok:\n",
    "            raise RuntimeError(\"TRT execute failed\")\n",
    "\n",
    "        pred = yhat.float().argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.shape[0]\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5be43fb-7d8f-445b-8bc2-27280ac2ce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2025-09:52:05] [TRT] [W] WARNING The logger passed into createInferBuilder differs from one already registered for an existing builder, runtime, or refitter. So the current new logger is ignored, and TensorRT will use the existing one which is returned by nvinfer1::getLogger() instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_5287623/ipykernel_2234128/307716010.py:91: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.\n",
      "  config.int8_calibrator = EntropyCalibrator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_391_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_421, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_418, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_415, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_412, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_409, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_406, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_403, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_400, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_397, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_394, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_388, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_385, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_382, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_379, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_376, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_373, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_370, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_367, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_364, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_361, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_325_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_355, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_352, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_349, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_346, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_343, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_340, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_337, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_334, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_331, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:06] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_328, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "Saved: resnet_qat_int8_b1.engine | cache: resnet_qat_int8_b1.cache | calib_batches=200\n",
      "[12/14/2025-09:52:30] [TRT] [W] WARNING The logger passed into createInferBuilder differs from one already registered for an existing builder, runtime, or refitter. So the current new logger is ignored, and TensorRT will use the existing one which is returned by nvinfer1::getLogger() instead.\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_391_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_421, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_418, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_415, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_412, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_409, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_406, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_403, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_400, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_397, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_394, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_388, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_385, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_382, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_379, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_376, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_373, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_370, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_367, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_364, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_361, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_325_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_355, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_352, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_349, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_346, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_343, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_340, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_337, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_334, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_331, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:30] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_328, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "Saved: resnet_qat_int8_b64.engine | cache: resnet_qat_int8_b64.cache | calib_batches=200\n",
      "[12/14/2025-09:52:52] [TRT] [W] WARNING The logger passed into createInferBuilder differs from one already registered for an existing builder, runtime, or refitter. So the current new logger is ignored, and TensorRT will use the existing one which is returned by nvinfer1::getLogger() instead.\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_391_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_421, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_418, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_415, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_412, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_409, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_406, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_403, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_400, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_397, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_394, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_358_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_388, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_385, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_382, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_379, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_376, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_373, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_370, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_367, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_364, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_361, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_325_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_355, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_352, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_349, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_346, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_343, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_340, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_337, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_334, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_331, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[12/14/2025-09:52:52] [TRT] [W] Missing scale and zero-point for tensor onnx::Conv_328, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "Saved: resnet_qat_int8_b128.engine | cache: resnet_qat_int8_b128.cache | calib_batches=200\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import torch\n",
    "import onnx\n",
    "\n",
    "# ---- QUIET logger (prevents per-batch spam) ----\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "# ---- YOUR 3 STATIC ONNX FILES (fixed batch) ----\n",
    "onnx_map = {\n",
    "    1:   \"resnet_qat_fp32_b1_op13.onnx\",\n",
    "    64:  \"resnet_qat_fp32_b64_op13.onnx\",\n",
    "    128: \"resnet_qat_fp32_b128_op13.onnx\",\n",
    "}\n",
    "\n",
    "# ---- CALIBRATION LOADERS MUST MATCH BATCH SIZE ----\n",
    "calib_loader_map = {\n",
    "    1:   test_loader_b1,\n",
    "    64:  test_loader_b64,\n",
    "    128: test_loader_b128,\n",
    "}\n",
    "\n",
    "CALIB_BATCHES = 200          # 200 is fine; 500 if you want extra\n",
    "WORKSPACE     = 1 << 28      # 256MB (raise to 1<<29 if build fails)\n",
    "\n",
    "# ---------------- Calibrator ----------------\n",
    "class EntropyCalibrator(trt.IInt8EntropyCalibrator2):\n",
    "    def __init__(self, calib_loader, max_batches=200, cache_file=\"calib.cache\"):\n",
    "        super().__init__()\n",
    "        self.cache_file = cache_file\n",
    "        self.data_iter = iter(calib_loader)\n",
    "        self.max_batches = max_batches\n",
    "        self.batch_count = 0\n",
    "\n",
    "        x0, _ = next(iter(calib_loader))\n",
    "        self.batch_size = x0.shape[0]\n",
    "        self.device_input = torch.empty_like(x0, device=\"cuda\")\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def get_batch(self, names):\n",
    "        if self.batch_count >= self.max_batches:\n",
    "            return None\n",
    "        try:\n",
    "            x, _ = next(self.data_iter)\n",
    "        except StopIteration:\n",
    "            return None\n",
    "\n",
    "        self.batch_count += 1\n",
    "        x = x.to(\"cuda\", non_blocking=True)\n",
    "        self.device_input.resize_(x.shape).copy_(x)\n",
    "        return [int(self.device_input.data_ptr())]\n",
    "\n",
    "    def read_calibration_cache(self):\n",
    "        try:\n",
    "            with open(self.cache_file, \"rb\") as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            return None\n",
    "\n",
    "    def write_calibration_cache(self, cache):\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            f.write(cache)\n",
    "\n",
    "# ---------------- Builder ----------------\n",
    "def build_int8_engine_static(onnx_path, engine_path, calib_loader, max_calib_batches=200):\n",
    "    # sanity check: if ONNX already has Q/DQ, you generally should NOT calibrate\n",
    "    m = onnx.load(onnx_path)\n",
    "    ops = set(n.op_type for n in m.graph.node)\n",
    "    has_qdq = (\"QuantizeLinear\" in ops) or (\"DequantizeLinear\" in ops)\n",
    "    if has_qdq:\n",
    "        print(f\"[WARN] {onnx_path} contains Quantize/Dequantize ops. \"\n",
    "              f\"PTQ calibration may be wrong for this file.\")\n",
    "\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "        with open(onnx_path, \"rb\") as f:\n",
    "            if not parser.parse(f.read()):\n",
    "                for i in range(parser.num_errors):\n",
    "                    print(parser.get_error(i))\n",
    "                raise RuntimeError(f\"ONNX parse failed for {onnx_path}\")\n",
    "\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, WORKSPACE)\n",
    "\n",
    "        # INT8 PTQ\n",
    "        config.set_flag(trt.BuilderFlag.INT8)\n",
    "        cache_file = engine_path.replace(\".engine\", \".cache\")\n",
    "        config.int8_calibrator = EntropyCalibrator(\n",
    "            calib_loader,\n",
    "            max_batches=max_calib_batches,\n",
    "            cache_file=cache_file\n",
    "        )\n",
    "\n",
    "        # static ONNX => NO optimization profile needed\n",
    "        serialized = builder.build_serialized_network(network, config)\n",
    "        if serialized is None:\n",
    "            raise RuntimeError(f\"INT8 engine build failed for {onnx_path}\")\n",
    "\n",
    "        with open(engine_path, \"wb\") as f:\n",
    "            f.write(serialized)\n",
    "\n",
    "    print(f\"Saved: {engine_path} | cache: {cache_file} | calib_batches={max_calib_batches}\")\n",
    "\n",
    "# ---------------- Build all 3 ----------------\n",
    "for bs, onnx_path in onnx_map.items():\n",
    "    build_int8_engine_static(\n",
    "        onnx_path=onnx_path,\n",
    "        engine_path=f\"resnet_qat_int8_b{bs}.engine\",\n",
    "        calib_loader=calib_loader_map[bs],\n",
    "        max_calib_batches=CALIB_BATCHES\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5e99fb4-c4f8-421f-ac0d-86fd5d08cf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 1.2M Dec 14 09:52 resnet_qat_int8_b1.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 856K Dec 14 09:52 resnet_qat_int8_b64.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 877K Dec 14 09:53 resnet_qat_int8_b128.engine\n"
     ]
    }
   ],
   "source": [
    "!ls -lh resnet_qat_int8_b1.engine\n",
    "!ls -lh resnet_qat_int8_b64.engine\n",
    "!ls -lh resnet_qat_int8_b128.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50ef2046-109c-45e9-8d6f-aa5cd7eb4e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2025-09:53:44] [TRT] [W] WARNING The logger passed into createInferRuntime differs from one already registered for an existing builder, runtime, or refitter. So the current new logger is ignored, and TensorRT will use the existing one which is returned by nvinfer1::getLogger() instead.\n",
      "[12/14/2025-09:53:44] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/14/2025-09:53:51] [TRT] [W] WARNING The logger passed into createInferRuntime differs from one already registered for an existing builder, runtime, or refitter. So the current new logger is ignored, and TensorRT will use the existing one which is returned by nvinfer1::getLogger() instead.\n",
      "[12/14/2025-09:53:51] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/14/2025-09:53:52] [TRT] [W] WARNING The logger passed into createInferRuntime differs from one already registered for an existing builder, runtime, or refitter. So the current new logger is ignored, and TensorRT will use the existing one which is returned by nvinfer1::getLogger() instead.\n",
      "[12/14/2025-09:53:52] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "INT8 TRT Acc b1:   10.03%\n",
      "INT8 TRT Acc b64:  9.84%\n",
      "INT8 TRT Acc b128: 9.88%\n"
     ]
    }
   ],
   "source": [
    "acc1   = trt_accuracy_static(\"resnet_qat_int8_b1.engine\",   test_loader_b1)\n",
    "acc64  = trt_accuracy_static(\"resnet_qat_int8_b64.engine\",  test_loader_b64)\n",
    "acc128 = trt_accuracy_static(\"resnet_qat_int8_b128.engine\", test_loader_b128)\n",
    "\n",
    "print(f\"INT8 TRT Acc b1:   {acc1:.2f}%\")\n",
    "print(f\"INT8 TRT Acc b64:  {acc64:.2f}%\")\n",
    "print(f\"INT8 TRT Acc b128: {acc128:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54ffe9cf-4ea9-4d56-9285-6c1ef58cee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch acc: 9.83\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def torch_acc(model, loader, device=\"cuda\"):\n",
    "    model.eval().to(device)\n",
    "    correct = total = 0\n",
    "    for x,y in loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        correct += (pred==y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return 100*correct/total\n",
    "\n",
    "print(\"Torch acc:\", torch_acc(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd04b1e-a34d-4db9-bea1-ae83cd7205b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_loader.dataset.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd5c0d-f755-4c20-a1e7-569de0092dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "m = onnx.load(\"resnet_qat_fp32_b1_op13.onnx\")  # <-- change to your qat onnx filename\n",
    "ops = sorted(set(n.op_type for n in m.graph.node))\n",
    "print(\"QuantizeLinear:\", \"QuantizeLinear\" in ops)\n",
    "print(\"DequantizeLinear:\", \"DequantizeLinear\" in ops)\n",
    "print(\"Num nodes:\", len(m.graph.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e90f80d-25c3-44c9-a142-0a0328c95b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "m = onnx.load(\"resnet_qat_fp32_b1_op13.onnx\")\n",
    "print(\"Inputs:\")\n",
    "for i in m.graph.input:\n",
    "    print(i.name)\n",
    "print(\"Outputs:\")\n",
    "for o in m.graph.output:\n",
    "    print(o.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd2d04-e175-494f-9ed1-b092e7014296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.ERROR)\n",
    "with open(\"resnet_qat_int8_b1.engine\",\"rb\") as f, trt.Runtime(TRT_LOGGER) as rt:\n",
    "    engine = rt.deserialize_cuda_engine(f.read())\n",
    "\n",
    "print(\"Engine name:\", engine.name)\n",
    "print(\"Has refittable:\", engine.refittable)\n",
    "print(\"Num layers:\", engine.num_layers)\n",
    "print(\"Has dynamic shapes:\", engine.num_optimization_profiles > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "868aa200-74a4-408f-bf31-641e344bdb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has QuantizeLinear? False\n",
      "Has DequantizeLinear? False\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "m = onnx.load(\"resnet_qat_fp32_b1_op13.onnx\")   # <-- whichever file you're using\n",
    "ops = sorted(set(n.op_type for n in m.graph.node))\n",
    "print(\"Has QuantizeLinear?\", \"QuantizeLinear\" in ops)\n",
    "print(\"Has DequantizeLinear?\", \"DequantizeLinear\" in ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c8c5f-143f-47ff-9513-03cc067b037c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee599 (PyTorch)",
   "language": "python",
   "name": "ee599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
