{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9fb18d4-66d7-421d-bf00-63460b5c60e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.9.1+cu128 CUDA: True\n",
      "TensorRT: 10.14.1.48.post1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())\n",
    "print(\"TensorRT:\", trt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e20c95b9-71ae-476f-9fe5-3997105ecf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parents[1]))\n",
    "\n",
    "from models.resnet32_model import ResNet\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd93386-e7a2-4028-8377-27899c33347a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet(num_classes=10)\n",
    "model.load_state_dict(\n",
    "    torch.load(\"../../pth/resnet_10.pth\", map_location=\"cpu\")\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ea8db1-83f3-4806-8401-14ac8fcedd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_5267562/ipykernel_91927/1696751656.py:12: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported resnet_10_b1_op13.onnx\n",
      "Exported resnet_10_b64_op13.onnx\n",
      "Exported resnet_10_b128_op13.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "dummy_map = {\n",
    "    1:   torch.randn(1,   3, 32, 32),\n",
    "    64:  torch.randn(64,  3, 32, 32),\n",
    "    128: torch.randn(128, 3, 32, 32),\n",
    "}\n",
    "\n",
    "for bs, dummy in dummy_map.items():\n",
    "    out_path = f\"resnet_10_b{bs}_op13.onnx\"\n",
    "    torch.onnx.export(\n",
    "        model, dummy, out_path,\n",
    "        opset_version=13,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes=None,   # <-- IMPORTANT: static\n",
    "        dynamo=False\n",
    "    )\n",
    "    print(\"Exported\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17143253-186e-4c27-ad4a-c67f1e4d6429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 1.8M Dec 13 19:03 resnet_10_b1_op13.onnx\n",
      "-rw-r--r-- 1 ihsiao ihsiao 1.8M Dec 13 19:03 resnet_10_b64_op13.onnx\n",
      "-rw-r--r-- 1 ihsiao ihsiao 1.8M Dec 13 19:03 resnet_10_b128_op13.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -lh resnet_10_b1_op13.onnx\n",
    "!ls -lh resnet_10_b64_op13.onnx\n",
    "!ls -lh resnet_10_b128_op13.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85a783e-e0c7-4e28-ae74-93c7507e8996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 13)]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "m = onnx.load(\"resnet_10_b1_op13.onnx\")\n",
    "onnx.checker.check_model(m)\n",
    "print([(op.domain, op.version) for op in m.opset_import])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ac89d5-edbc-4178-b9e1-8f408f840f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.14.1.48.post1\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "print(trt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c0c6a83-4e52-4242-aea3-229a1256bc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2025-19:03:43] [TRT] [I] [MemUsageChange] Init CUDA: CPU -23, GPU +0, now: CPU 347, GPU 422 (MiB)\n",
      "[12/13/2025-19:03:43] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:03:43] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/13/2025-19:03:43] [TRT] [I] Opset version:    13\n",
      "[12/13/2025-19:03:43] [TRT] [I] Producer name:    pytorch\n",
      "[12/13/2025-19:03:43] [TRT] [I] Producer version: 2.9.1\n",
      "[12/13/2025-19:03:43] [TRT] [I] Domain:           \n",
      "[12/13/2025-19:03:43] [TRT] [I] Model version:    0\n",
      "[12/13/2025-19:03:43] [TRT] [I] Doc string:       \n",
      "[12/13/2025-19:03:43] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:03:44] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +283, GPU +8, now: CPU 669, GPU 430 (MiB)\n",
      "[12/13/2025-19:03:44] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/13/2025-19:03:51] [TRT] [I] Compiler backend is used during engine build.\n",
      "[12/13/2025-19:03:52] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/13/2025-19:03:52] [TRT] [I] Total Host Persistent Memory: 188784 bytes\n",
      "[12/13/2025-19:03:52] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/13/2025-19:03:52] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/13/2025-19:03:52] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 36 steps to complete.\n",
      "[12/13/2025-19:03:52] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.208581ms to assign 3 blocks to 36 nodes requiring 196608 bytes.\n",
      "[12/13/2025-19:03:52] [TRT] [I] Total Activation Memory: 196608 bytes\n",
      "[12/13/2025-19:03:52] [TRT] [I] Total Weights Memory: 1877032 bytes\n",
      "[12/13/2025-19:03:52] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[12/13/2025-19:03:52] [TRT] [I] Engine generation completed in 8.53988 seconds.\n",
      "[12/13/2025-19:03:52] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 6 MiB\n",
      "Saved: resnet_10_b1.engine\n",
      "[12/13/2025-19:03:52] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:03:52] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/13/2025-19:03:52] [TRT] [I] Opset version:    13\n",
      "[12/13/2025-19:03:52] [TRT] [I] Producer name:    pytorch\n",
      "[12/13/2025-19:03:52] [TRT] [I] Producer version: 2.9.1\n",
      "[12/13/2025-19:03:52] [TRT] [I] Domain:           \n",
      "[12/13/2025-19:03:52] [TRT] [I] Model version:    0\n",
      "[12/13/2025-19:03:52] [TRT] [I] Doc string:       \n",
      "[12/13/2025-19:03:52] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:03:53] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +2, now: CPU 2727, GPU 682 (MiB)\n",
      "[12/13/2025-19:03:53] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/13/2025-19:03:59] [TRT] [I] Compiler backend is used during engine build.\n",
      "[12/13/2025-19:04:00] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/13/2025-19:04:01] [TRT] [I] Total Host Persistent Memory: 187120 bytes\n",
      "[12/13/2025-19:04:01] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/13/2025-19:04:01] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/13/2025-19:04:01] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 36 steps to complete.\n",
      "[12/13/2025-19:04:01] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.196929ms to assign 3 blocks to 36 nodes requiring 12582912 bytes.\n",
      "[12/13/2025-19:04:01] [TRT] [I] Total Activation Memory: 12582912 bytes\n",
      "[12/13/2025-19:04:01] [TRT] [I] Total Weights Memory: 1877376 bytes\n",
      "[12/13/2025-19:04:01] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[12/13/2025-19:04:01] [TRT] [I] Engine generation completed in 8.03503 seconds.\n",
      "[12/13/2025-19:04:01] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 13 MiB\n",
      "Saved: resnet_10_b64.engine\n",
      "[12/13/2025-19:04:01] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:04:01] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/13/2025-19:04:01] [TRT] [I] Opset version:    13\n",
      "[12/13/2025-19:04:01] [TRT] [I] Producer name:    pytorch\n",
      "[12/13/2025-19:04:01] [TRT] [I] Producer version: 2.9.1\n",
      "[12/13/2025-19:04:01] [TRT] [I] Domain:           \n",
      "[12/13/2025-19:04:01] [TRT] [I] Model version:    0\n",
      "[12/13/2025-19:04:01] [TRT] [I] Doc string:       \n",
      "[12/13/2025-19:04:01] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:04:01] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +2, now: CPU 2760, GPU 684 (MiB)\n",
      "[12/13/2025-19:04:01] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/13/2025-19:04:08] [TRT] [I] Compiler backend is used during engine build.\n",
      "[12/13/2025-19:04:09] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/13/2025-19:04:09] [TRT] [I] Total Host Persistent Memory: 188976 bytes\n",
      "[12/13/2025-19:04:09] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/13/2025-19:04:09] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/13/2025-19:04:09] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 37 steps to complete.\n",
      "[12/13/2025-19:04:09] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.23512ms to assign 4 blocks to 37 nodes requiring 25166336 bytes.\n",
      "[12/13/2025-19:04:09] [TRT] [I] Total Activation Memory: 25165824 bytes\n",
      "[12/13/2025-19:04:09] [TRT] [I] Total Weights Memory: 1876864 bytes\n",
      "[12/13/2025-19:04:09] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[12/13/2025-19:04:09] [TRT] [I] Engine generation completed in 7.96197 seconds.\n",
      "[12/13/2025-19:04:09] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 25 MiB\n",
      "Saved: resnet_10_b128.engine\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "\n",
    "# IMPORTANT: these ONNX files must be exported with FIXED batch sizes (static)\n",
    "onnx_map = {\n",
    "    1:   \"resnet_10_b1_op13.onnx\",\n",
    "    64:  \"resnet_10_b64_op13.onnx\",\n",
    "    128: \"resnet_10_b128_op13.onnx\",\n",
    "}\n",
    "\n",
    "def build_static_engine(onnx_path, engine_path):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "        with open(onnx_path, \"rb\") as f:\n",
    "            if not parser.parse(f.read()):\n",
    "                for i in range(parser.num_errors):\n",
    "                    print(parser.get_error(i))\n",
    "                raise RuntimeError(f\"ONNX parse failed for {onnx_path}\")\n",
    "\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)\n",
    "\n",
    "        # NO optimization profile => static engine (uses whatever fixed shape is in ONNX)\n",
    "        serialized = builder.build_serialized_network(network, config)\n",
    "        if serialized is None:\n",
    "            raise RuntimeError(f\"Engine build failed for {onnx_path}\")\n",
    "\n",
    "        with open(engine_path, \"wb\") as f:\n",
    "            f.write(serialized)\n",
    "\n",
    "    print(\"Saved:\", engine_path)\n",
    "\n",
    "for bs, onnx_path in onnx_map.items():\n",
    "    engine_path = f\"resnet_10_b{bs}.engine\"\n",
    "    build_static_engine(onnx_path, engine_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37bac398-e290-467b-bf6c-a49cc60b1cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 2.0M Dec 13 19:03 resnet_10_b1.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 2.1M Dec 13 19:04 resnet_10_b64.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 2.1M Dec 13 19:04 resnet_10_b128.engine\n"
     ]
    }
   ],
   "source": [
    "!ls -lh resnet_10_b1.engine\n",
    "!ls -lh resnet_10_b64.engine\n",
    "!ls -lh resnet_10_b128.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa592380-cf3a-43a7-bfdf-19ee4c109118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmark (3 static engines)...\n",
      "resnet_10_b1.engine | batch=1\n",
      "  latency:     0.310 ms/batch\n",
      "  per-image:   0.309639 ms/image\n",
      "  throughput:  3229.6 images/sec\n",
      "resnet_10_b64.engine | batch=64\n",
      "  latency:     0.509 ms/batch\n",
      "  per-image:   0.007956 ms/image\n",
      "  throughput:  125688.3 images/sec\n",
      "resnet_10_b128.engine | batch=128\n",
      "  latency:     0.726 ms/batch\n",
      "  per-image:   0.005671 ms/image\n",
      "  throughput:  176323.8 images/sec\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import torch\n",
    "\n",
    "def benchmark_engine_static(engine_path, batch_size, iters=1000):\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "    with open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
    "    inp = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.INPUT][0]\n",
    "    out = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.OUTPUT][0]\n",
    "\n",
    "    # ✅ STATIC engine: DO NOT set_input_shape()\n",
    "    # context.set_input_shape(inp, (batch_size, 3, 32, 32))\n",
    "\n",
    "    x = torch.randn(batch_size, 3, 32, 32, device=\"cuda\", dtype=torch.float32)\n",
    "    y = torch.empty(tuple(context.get_tensor_shape(out)), device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "    context.set_tensor_address(inp, int(x.data_ptr()))\n",
    "    context.set_tensor_address(out, int(y.data_ptr()))\n",
    "\n",
    "    stream = torch.cuda.Stream()\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(50):\n",
    "        context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end   = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start.record(stream)\n",
    "    for _ in range(iters):\n",
    "        context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "    end.record(stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "    elapsed_ms = start.elapsed_time(end)\n",
    "\n",
    "    batch_latency_ms = elapsed_ms / iters\n",
    "    throughput = (iters * batch_size) / (elapsed_ms / 1000.0)\n",
    "    ms_per_img = batch_latency_ms / batch_size\n",
    "\n",
    "    print(f\"{engine_path} | batch={batch_size}\")\n",
    "    print(f\"  latency:     {batch_latency_ms:.3f} ms/batch\")\n",
    "    print(f\"  per-image:   {ms_per_img:.6f} ms/image\")\n",
    "    print(f\"  throughput:  {throughput:.1f} images/sec\")\n",
    "\n",
    "\n",
    "print(\"Starting benchmark (3 static engines)...\")\n",
    "benchmark_engine_static(\"resnet_10_b1.engine\",   batch_size=1,   iters=1000)\n",
    "benchmark_engine_static(\"resnet_10_b64.engine\",  batch_size=64,  iters=1000)\n",
    "benchmark_engine_static(\"resnet_10_b128.engine\", batch_size=128, iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1dfa043-1872-4920-873f-04560db1663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2025-19:04:12] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:04:12] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/13/2025-19:04:12] [TRT] [I] Opset version:    13\n",
      "[12/13/2025-19:04:12] [TRT] [I] Producer name:    pytorch\n",
      "[12/13/2025-19:04:12] [TRT] [I] Producer version: 2.9.1\n",
      "[12/13/2025-19:04:12] [TRT] [I] Domain:           \n",
      "[12/13/2025-19:04:12] [TRT] [I] Model version:    0\n",
      "[12/13/2025-19:04:12] [TRT] [I] Doc string:       \n",
      "[12/13/2025-19:04:12] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:04:12] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +0, now: CPU 2749, GPU 728 (MiB)\n",
      "[12/13/2025-19:04:12] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/13/2025-19:04:27] [TRT] [I] Compiler backend is used during engine build.\n",
      "[12/13/2025-19:04:30] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/13/2025-19:04:31] [TRT] [I] Total Host Persistent Memory: 186288 bytes\n",
      "[12/13/2025-19:04:31] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/13/2025-19:04:31] [TRT] [I] Max Scratch Memory: 512 bytes\n",
      "[12/13/2025-19:04:31] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 36 steps to complete.\n",
      "[12/13/2025-19:04:31] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.220583ms to assign 3 blocks to 36 nodes requiring 98304 bytes.\n",
      "[12/13/2025-19:04:31] [TRT] [I] Total Activation Memory: 98304 bytes\n",
      "[12/13/2025-19:04:31] [TRT] [I] Total Weights Memory: 947988 bytes\n",
      "[12/13/2025-19:04:31] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[12/13/2025-19:04:31] [TRT] [I] Engine generation completed in 18.4828 seconds.\n",
      "[12/13/2025-19:04:31] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 25 MiB\n",
      "Saved: resnet_10_fp16_b1.engine\n",
      "[12/13/2025-19:04:31] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:04:31] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/13/2025-19:04:31] [TRT] [I] Opset version:    13\n",
      "[12/13/2025-19:04:31] [TRT] [I] Producer name:    pytorch\n",
      "[12/13/2025-19:04:31] [TRT] [I] Producer version: 2.9.1\n",
      "[12/13/2025-19:04:31] [TRT] [I] Domain:           \n",
      "[12/13/2025-19:04:31] [TRT] [I] Model version:    0\n",
      "[12/13/2025-19:04:31] [TRT] [I] Doc string:       \n",
      "[12/13/2025-19:04:31] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:04:31] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +0, now: CPU 2780, GPU 732 (MiB)\n",
      "[12/13/2025-19:04:31] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/13/2025-19:04:45] [TRT] [I] Compiler backend is used during engine build.\n",
      "[12/13/2025-19:04:48] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/13/2025-19:04:48] [TRT] [I] Total Host Persistent Memory: 183856 bytes\n",
      "[12/13/2025-19:04:48] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/13/2025-19:04:48] [TRT] [I] Max Scratch Memory: 1536 bytes\n",
      "[12/13/2025-19:04:48] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 36 steps to complete.\n",
      "[12/13/2025-19:04:48] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.213671ms to assign 3 blocks to 36 nodes requiring 6291456 bytes.\n",
      "[12/13/2025-19:04:48] [TRT] [I] Total Activation Memory: 6291456 bytes\n",
      "[12/13/2025-19:04:48] [TRT] [I] Total Weights Memory: 948352 bytes\n",
      "[12/13/2025-19:04:48] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[12/13/2025-19:04:48] [TRT] [I] Engine generation completed in 16.8315 seconds.\n",
      "[12/13/2025-19:04:48] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 25 MiB\n",
      "Saved: resnet_10_fp16_b64.engine\n",
      "[12/13/2025-19:04:48] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:04:48] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/13/2025-19:04:48] [TRT] [I] Opset version:    13\n",
      "[12/13/2025-19:04:48] [TRT] [I] Producer name:    pytorch\n",
      "[12/13/2025-19:04:48] [TRT] [I] Producer version: 2.9.1\n",
      "[12/13/2025-19:04:48] [TRT] [I] Domain:           \n",
      "[12/13/2025-19:04:48] [TRT] [I] Model version:    0\n",
      "[12/13/2025-19:04:48] [TRT] [I] Doc string:       \n",
      "[12/13/2025-19:04:48] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:04:49] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +0, now: CPU 2782, GPU 734 (MiB)\n",
      "[12/13/2025-19:04:49] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/13/2025-19:05:02] [TRT] [I] Compiler backend is used during engine build.\n",
      "[12/13/2025-19:05:05] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/13/2025-19:05:05] [TRT] [I] Total Host Persistent Memory: 188976 bytes\n",
      "[12/13/2025-19:05:05] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/13/2025-19:05:05] [TRT] [I] Max Scratch Memory: 2560 bytes\n",
      "[12/13/2025-19:05:05] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 36 steps to complete.\n",
      "[12/13/2025-19:05:05] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.209533ms to assign 3 blocks to 36 nodes requiring 12582912 bytes.\n",
      "[12/13/2025-19:05:05] [TRT] [I] Total Activation Memory: 12582912 bytes\n",
      "[12/13/2025-19:05:05] [TRT] [I] Total Weights Memory: 948352 bytes\n",
      "[12/13/2025-19:05:05] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[12/13/2025-19:05:05] [TRT] [I] Engine generation completed in 16.4393 seconds.\n",
      "[12/13/2025-19:05:05] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 25 MiB\n",
      "Saved: resnet_10_fp16_b128.engine\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "\n",
    "# IMPORTANT: these ONNX files must be exported with FIXED batch sizes (static)\n",
    "onnx_map = {\n",
    "    1:   \"resnet_10_b1_op13.onnx\",\n",
    "    64:  \"resnet_10_b64_op13.onnx\",\n",
    "    128: \"resnet_10_b128_op13.onnx\",\n",
    "}\n",
    "\n",
    "def build_static_fp16_engine(onnx_path, engine_path):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "        with open(onnx_path, \"rb\") as f:\n",
    "            if not parser.parse(f.read()):\n",
    "                for i in range(parser.num_errors):\n",
    "                    print(parser.get_error(i))\n",
    "                raise RuntimeError(f\"ONNX parse failed for {onnx_path}\")\n",
    "\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)\n",
    "\n",
    "        # ✅ FP16 enabled\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "\n",
    "        # ✅ NO optimization profile => static engine (fixed shape from ONNX)\n",
    "        serialized = builder.build_serialized_network(network, config)\n",
    "        if serialized is None:\n",
    "            raise RuntimeError(f\"FP16 engine build failed for {onnx_path}\")\n",
    "\n",
    "        with open(engine_path, \"wb\") as f:\n",
    "            f.write(serialized)\n",
    "\n",
    "    print(\"Saved:\", engine_path)\n",
    "\n",
    "for bs, onnx_path in onnx_map.items():\n",
    "    engine_path = f\"resnet_10_fp16_b{bs}.engine\"\n",
    "    build_static_fp16_engine(onnx_path, engine_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dff36bb1-0473-480e-96e4-239d24e4e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 1.2M Dec 13 19:04 resnet_10_fp16_b1.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 1.2M Dec 13 19:04 resnet_10_fp16_b64.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 1.2M Dec 13 19:05 resnet_10_fp16_b128.engine\n"
     ]
    }
   ],
   "source": [
    "!ls -lh resnet_10_fp16_b1.engine\n",
    "!ls -lh resnet_10_fp16_b64.engine\n",
    "!ls -lh resnet_10_fp16_b128.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eb914e1-f19a-42c2-b8e4-04ae245d5766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet_10_fp16_b1.engine | batch=1\n",
      "  batch latency: 0.246 ms/batch\n",
      "  per-image:     0.245730 ms/image\n",
      "  throughput:    4069.5 images/sec\n",
      "resnet_10_fp16_b64.engine | batch=64\n",
      "  batch latency: 0.328 ms/batch\n",
      "  per-image:     0.005124 ms/image\n",
      "  throughput:    195174.6 images/sec\n",
      "resnet_10_fp16_b128.engine | batch=128\n",
      "  batch latency: 0.433 ms/batch\n",
      "  per-image:     0.003383 ms/image\n",
      "  throughput:    295599.8 images/sec\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import torch\n",
    "\n",
    "def run_engine_static(engine_path, batch, iters=1000, warmup=50):\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "    with open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    tensor_names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
    "    inp_name  = [n for n in tensor_names if engine.get_tensor_mode(n) == trt.TensorIOMode.INPUT][0]\n",
    "    out_name  = [n for n in tensor_names if engine.get_tensor_mode(n) == trt.TensorIOMode.OUTPUT][0]\n",
    "\n",
    "    # ❌ NO set_input_shape() for static engines\n",
    "\n",
    "    x = torch.randn(batch, 3, 32, 32, device=\"cuda\", dtype=torch.float32)\n",
    "    y = torch.empty(tuple(context.get_tensor_shape(out_name)), device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "    context.set_tensor_address(inp_name, int(x.data_ptr()))\n",
    "    context.set_tensor_address(out_name, int(y.data_ptr()))\n",
    "\n",
    "    stream = torch.cuda.Stream()\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(warmup):\n",
    "        context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end   = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start.record(stream)\n",
    "    for _ in range(iters):\n",
    "        context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "    end.record(stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "    elapsed_ms = start.elapsed_time(end)\n",
    "    batch_latency_ms = elapsed_ms / iters\n",
    "    ms_per_img = batch_latency_ms / batch\n",
    "    img_per_sec = (iters * batch) / (elapsed_ms / 1000.0)\n",
    "\n",
    "    print(f\"{engine_path} | batch={batch}\")\n",
    "    print(f\"  batch latency: {batch_latency_ms:.3f} ms/batch\")\n",
    "    print(f\"  per-image:     {ms_per_img:.6f} ms/image\")\n",
    "    print(f\"  throughput:    {img_per_sec:.1f} images/sec\")\n",
    "\n",
    "run_engine_static(\"resnet_10_fp16_b1.engine\",   1)\n",
    "run_engine_static(\"resnet_10_fp16_b64.engine\",  64)\n",
    "run_engine_static(\"resnet_10_fp16_b128.engine\", 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "250f0dd1-9496-4b88-add3-6ab7503cd722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57.7%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53f3b54e-aac2-4547-af94-14fdf8d9f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader_b1   = DataLoader(test_dataset, batch_size=1,   shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\n",
    "test_loader_b64  = DataLoader(test_dataset, batch_size=64,  shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\n",
    "test_loader_b128 = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcb0ea02-1c28-416f-917b-48b6789cd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "@torch.no_grad()\n",
    "def trt_accuracy_static(engine_path, test_loader, num_batches=None):\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "    with open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
    "    inp = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.INPUT][0]\n",
    "    out = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.OUTPUT][0]\n",
    "\n",
    "    # engine fixed shapes\n",
    "    in_shape = tuple(engine.get_tensor_shape(inp))\n",
    "    out_shape = tuple(engine.get_tensor_shape(out))\n",
    "    fixed_bsz = in_shape[0]  # should be 1 or 64 or 128\n",
    "\n",
    "    # output dtype\n",
    "    trt_dtype = engine.get_tensor_dtype(out)\n",
    "    torch_dtype = {\n",
    "        trt.DataType.FLOAT: torch.float32,\n",
    "        trt.DataType.HALF:  torch.float16,\n",
    "        trt.DataType.INT8:  torch.int8,\n",
    "        trt.DataType.INT32: torch.int32,\n",
    "    }[trt_dtype]\n",
    "\n",
    "    stream = torch.cuda.current_stream()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for bi, (x_cpu, y_cpu) in enumerate(test_loader):\n",
    "        if num_batches is not None and bi >= num_batches:\n",
    "            break\n",
    "\n",
    "        x = x_cpu.to(\"cuda\", non_blocking=True)\n",
    "        y = y_cpu.to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        if x.shape[0] != fixed_bsz:\n",
    "            raise RuntimeError(f\"Batch mismatch: loader={x.shape[0]} but engine expects {fixed_bsz}\")\n",
    "\n",
    "        yhat = torch.empty(out_shape, device=\"cuda\", dtype=torch_dtype)\n",
    "\n",
    "        context.set_tensor_address(inp, int(x.data_ptr()))\n",
    "        context.set_tensor_address(out, int(yhat.data_ptr()))\n",
    "\n",
    "        ok = context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "        if not ok:\n",
    "            raise RuntimeError(\"TRT execute failed\")\n",
    "\n",
    "        pred = yhat.float().argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.shape[0]\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15e04cc8-22e8-41d7-9345-d5bd98464ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2025-19:05:24] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/13/2025-19:05:36] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/13/2025-19:05:37] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "FP32 TRT Acc b1:   90.38%\n",
      "FP32 TRT Acc b64:  90.37%\n",
      "FP32 TRT Acc b128: 90.38%\n"
     ]
    }
   ],
   "source": [
    "acc1   = trt_accuracy_static(\"resnet_10_b1.engine\",   test_loader_b1)\n",
    "acc64  = trt_accuracy_static(\"resnet_10_b64.engine\",  test_loader_b64)\n",
    "acc128 = trt_accuracy_static(\"resnet_10_b128.engine\", test_loader_b128)\n",
    "\n",
    "print(f\"FP32 TRT Acc b1:   {acc1:.2f}%\")\n",
    "print(f\"FP32 TRT Acc b64:  {acc64:.2f}%\")\n",
    "print(f\"FP32 TRT Acc b128: {acc128:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90f820f0-364f-4320-93e4-b90ed0965239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2025-19:05:38] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/13/2025-19:05:50] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/13/2025-19:05:51] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "FP16 TRT Acc b1:   90.39%\n",
      "FP16 TRT Acc b64:  90.39%\n",
      "FP16 TRT Acc b128: 90.39%\n"
     ]
    }
   ],
   "source": [
    "acc1   = trt_accuracy_static(\"resnet_10_fp16_b1.engine\",   test_loader_b1)\n",
    "acc64  = trt_accuracy_static(\"resnet_10_fp16_b64.engine\",  test_loader_b64)\n",
    "acc128 = trt_accuracy_static(\"resnet_10_fp16_b128.engine\", test_loader_b128)\n",
    "\n",
    "print(f\"FP16 TRT Acc b1:   {acc1:.2f}%\")\n",
    "print(f\"FP16 TRT Acc b64:  {acc64:.2f}%\")\n",
    "print(f\"FP16 TRT Acc b128: {acc128:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5be43fb-7d8f-445b-8bc2-27280ac2ce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_5267562/ipykernel_91927/401825817.py:69: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.\n",
      "  config.int8_calibrator = EntropyCalibrator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: resnet_10_int8_b1.engine (calib_batches=200)\n",
      "Saved: resnet_10_int8_b64.engine (calib_batches=200)\n",
      "Saved: resnet_10_int8_b128.engine (calib_batches=200)\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import torch\n",
    "\n",
    "# ✅ reduce spam\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "# ✅ use your 3 fixed-shape ONNX files\n",
    "onnx_map = {\n",
    "    1:   \"resnet_10_b1_op13.onnx\",\n",
    "    64:  \"resnet_10_b64_op13.onnx\",\n",
    "    128: \"resnet_10_b128_op13.onnx\",\n",
    "}\n",
    "\n",
    "class EntropyCalibrator(trt.IInt8EntropyCalibrator2):\n",
    "    def __init__(self, calib_loader, max_batches=200, cache_file=\"calib.cache\"):\n",
    "        super().__init__()\n",
    "        self.cache_file = cache_file\n",
    "        self.data_iter = iter(calib_loader)\n",
    "        self.max_batches = max_batches\n",
    "        self.batch_count = 0\n",
    "\n",
    "        x0, _ = next(iter(calib_loader))\n",
    "        self.batch_size = x0.shape[0]\n",
    "        self.device_input = torch.empty_like(x0, device=\"cuda\")\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def get_batch(self, names):\n",
    "        if self.batch_count >= self.max_batches:\n",
    "            return None\n",
    "        try:\n",
    "            x, _ = next(self.data_iter)\n",
    "        except StopIteration:\n",
    "            return None\n",
    "\n",
    "        self.batch_count += 1\n",
    "        x = x.to(\"cuda\", non_blocking=True)\n",
    "        self.device_input.resize_(x.shape).copy_(x)\n",
    "        return [int(self.device_input.data_ptr())]\n",
    "\n",
    "    def read_calibration_cache(self):\n",
    "        try:\n",
    "            with open(self.cache_file, \"rb\") as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            return None\n",
    "\n",
    "    def write_calibration_cache(self, cache):\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            f.write(cache)\n",
    "\n",
    "def build_int8_engine_static(onnx_path, engine_path, calib_loader, max_calib_batches=200):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "        with open(onnx_path, \"rb\") as f:\n",
    "            if not parser.parse(f.read()):\n",
    "                for i in range(parser.num_errors):\n",
    "                    print(parser.get_error(i))\n",
    "                raise RuntimeError(f\"ONNX parse failed for {onnx_path}\")\n",
    "\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)\n",
    "\n",
    "        # INT8 PTQ\n",
    "        config.set_flag(trt.BuilderFlag.INT8)\n",
    "        config.int8_calibrator = EntropyCalibrator(\n",
    "            calib_loader,\n",
    "            max_batches=max_calib_batches,\n",
    "            cache_file=engine_path.replace(\".engine\", \".cache\")\n",
    "        )\n",
    "\n",
    "        # ✅ static ONNX => NO optimization profile needed\n",
    "        serialized = builder.build_serialized_network(network, config)\n",
    "        if serialized is None:\n",
    "            raise RuntimeError(f\"INT8 Engine build failed for {onnx_path}\")\n",
    "\n",
    "        with open(engine_path, \"wb\") as f:\n",
    "            f.write(serialized)\n",
    "\n",
    "    print(f\"Saved: {engine_path} (calib_batches={max_calib_batches})\")\n",
    "\n",
    "# ✅ make sure these match the ONNX batch size:\n",
    "calib_loader_map = {\n",
    "    1:   test_loader_b1,\n",
    "    64:  test_loader_b64,\n",
    "    128: test_loader_b128,\n",
    "}\n",
    "\n",
    "CALIB_BATCHES = 200\n",
    "\n",
    "for bs, onnx_path in onnx_map.items():\n",
    "    build_int8_engine_static(\n",
    "        onnx_path=onnx_path,\n",
    "        engine_path=f\"resnet_10_int8_b{bs}.engine\",\n",
    "        calib_loader=calib_loader_map[bs],\n",
    "        max_calib_batches=CALIB_BATCHES\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5e99fb4-c4f8-421f-ac0d-86fd5d08cf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 973K Dec 13 19:06 resnet_10_int8_b1.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 824K Dec 13 19:06 resnet_10_int8_b64.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 786K Dec 13 19:07 resnet_10_int8_b128.engine\n"
     ]
    }
   ],
   "source": [
    "!ls -lh resnet_10_int8_b1.engine\n",
    "!ls -lh resnet_10_int8_b64.engine\n",
    "!ls -lh resnet_10_int8_b128.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50ef2046-109c-45e9-8d6f-aa5cd7eb4e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2025-19:07:25] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/13/2025-19:07:36] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/13/2025-19:07:37] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "INT8 TRT Acc b1:   90.30%\n",
      "INT8 TRT Acc b64:  90.35%\n",
      "INT8 TRT Acc b128: 90.36%\n"
     ]
    }
   ],
   "source": [
    "acc1   = trt_accuracy_static(\"resnet_10_int8_b1.engine\",   test_loader_b1)\n",
    "acc64  = trt_accuracy_static(\"resnet_10_int8_b64.engine\",  test_loader_b64)\n",
    "acc128 = trt_accuracy_static(\"resnet_10_int8_b128.engine\", test_loader_b128)\n",
    "\n",
    "print(f\"INT8 TRT Acc b1:   {acc1:.2f}%\")\n",
    "print(f\"INT8 TRT Acc b64:  {acc64:.2f}%\")\n",
    "print(f\"INT8 TRT Acc b128: {acc128:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54ffe9cf-4ea9-4d56-9285-6c1ef58cee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch acc: 90.4\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def torch_acc(model, loader, device=\"cuda\"):\n",
    "    model.eval().to(device)\n",
    "    correct = total = 0\n",
    "    for x,y in loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        correct += (pred==y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return 100*correct/total\n",
    "\n",
    "print(\"Torch acc:\", torch_acc(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd5c0d-f755-4c20-a1e7-569de0092dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee599 (PyTorch)",
   "language": "python",
   "name": "ee599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
