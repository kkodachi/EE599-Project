{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9fb18d4-66d7-421d-bf00-63460b5c60e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.9.1+cu128 CUDA: True\n",
      "TensorRT: 10.14.1.48.post1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())\n",
    "print(\"TensorRT:\", trt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e20c95b9-71ae-476f-9fe5-3997105ecf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parents[1]))\n",
    "\n",
    "from models.squeezenet_model import SqueezeNetCIFAR10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd93386-e7a2-4028-8377-27899c33347a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNetCIFAR10(\n",
       "  (conv1): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fire2): Fire(\n",
       "    (conv1): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fire3): Fire(\n",
       "    (conv1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fire4): Fire(\n",
       "    (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (dropout4): Dropout(p=0.2, inplace=False)\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fire5): Fire(\n",
       "    (conv1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fire6): Fire(\n",
       "    (conv1): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fire7): Fire(\n",
       "    (conv1): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fire8): Fire(\n",
       "    (conv1): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (dropout8): Dropout(p=0.2, inplace=False)\n",
       "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fire9): Fire(\n",
       "    (conv1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv10): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (dropout_final): Dropout(p=0.5, inplace=False)\n",
       "  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SqueezeNetCIFAR10(num_classes=10)\n",
    "model.load_state_dict(\n",
    "    torch.load(\"../../pth/squeezenet_fp32.pth\", map_location=\"cpu\")\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ea8db1-83f3-4806-8401-14ac8fcedd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_5267562/ipykernel_97961/1600352956.py:12: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported squeezenet_fp32_b1_op13.onnx\n",
      "Exported squeezenet_fp32_b64_op13.onnx\n",
      "Exported squeezenet_fp32_b128_op13.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "dummy_map = {\n",
    "    1:   torch.randn(1,   3, 32, 32),\n",
    "    64:  torch.randn(64,  3, 32, 32),\n",
    "    128: torch.randn(128, 3, 32, 32),\n",
    "}\n",
    "\n",
    "for bs, dummy in dummy_map.items():\n",
    "    out_path = f\"squeezenet_fp32_b{bs}_op13.onnx\"\n",
    "    torch.onnx.export(\n",
    "        model, dummy, out_path,\n",
    "        opset_version=13,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes=None,   # <-- IMPORTANT: static\n",
    "        dynamo=False\n",
    "    )\n",
    "    print(\"Exported\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17143253-186e-4c27-ad4a-c67f1e4d6429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 2.8M Dec 13 19:44 squeezenet_fp32_b1_op13.onnx\n",
      "-rw-r--r-- 1 ihsiao ihsiao 2.8M Dec 13 19:44 squeezenet_fp32_b64_op13.onnx\n",
      "-rw-r--r-- 1 ihsiao ihsiao 2.8M Dec 13 19:44 squeezenet_fp32_b128_op13.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -lh squeezenet_fp32_b1_op13.onnx\n",
    "!ls -lh squeezenet_fp32_b64_op13.onnx\n",
    "!ls -lh squeezenet_fp32_b128_op13.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85a783e-e0c7-4e28-ae74-93c7507e8996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 13)]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "m = onnx.load(\"squeezenet_fp32_b1_op13.onnx\")\n",
    "onnx.checker.check_model(m)\n",
    "print([(op.domain, op.version) for op in m.opset_import])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ac89d5-edbc-4178-b9e1-8f408f840f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.14.1.48.post1\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "print(trt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c0c6a83-4e52-4242-aea3-229a1256bc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2025-19:44:44] [TRT] [I] [MemUsageChange] Init CUDA: CPU -23, GPU +0, now: CPU 534, GPU 422 (MiB)\n",
      "[12/13/2025-19:44:45] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:44:45] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/13/2025-19:44:45] [TRT] [I] Opset version:    13\n",
      "[12/13/2025-19:44:45] [TRT] [I] Producer name:    pytorch\n",
      "[12/13/2025-19:44:45] [TRT] [I] Producer version: 2.9.1\n",
      "[12/13/2025-19:44:45] [TRT] [I] Domain:           \n",
      "[12/13/2025-19:44:45] [TRT] [I] Model version:    0\n",
      "[12/13/2025-19:44:45] [TRT] [I] Doc string:       \n",
      "[12/13/2025-19:44:45] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:44:45] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +281, GPU +8, now: CPU 672, GPU 430 (MiB)\n",
      "[12/13/2025-19:44:45] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/13/2025-19:45:17] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/13/2025-19:45:18] [TRT] [I] Total Host Persistent Memory: 194496 bytes\n",
      "[12/13/2025-19:45:18] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/13/2025-19:45:18] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/13/2025-19:45:18] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 33 steps to complete.\n",
      "[12/13/2025-19:45:18] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.159288ms to assign 3 blocks to 33 nodes requiring 671744 bytes.\n",
      "[12/13/2025-19:45:18] [TRT] [I] Total Activation Memory: 671744 bytes\n",
      "[12/13/2025-19:45:18] [TRT] [I] Total Weights Memory: 2928680 bytes\n",
      "[12/13/2025-19:45:18] [TRT] [I] Engine generation completed in 32.7835 seconds.\n",
      "[12/13/2025-19:45:18] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 7 MiB\n",
      "Saved: squeezenet_fp32_b1.engine\n",
      "[12/13/2025-19:45:18] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:45:18] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/13/2025-19:45:18] [TRT] [I] Opset version:    13\n",
      "[12/13/2025-19:45:18] [TRT] [I] Producer name:    pytorch\n",
      "[12/13/2025-19:45:18] [TRT] [I] Producer version: 2.9.1\n",
      "[12/13/2025-19:45:18] [TRT] [I] Domain:           \n",
      "[12/13/2025-19:45:18] [TRT] [I] Model version:    0\n",
      "[12/13/2025-19:45:18] [TRT] [I] Doc string:       \n",
      "[12/13/2025-19:45:18] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:45:18] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +2, now: CPU 1074, GPU 718 (MiB)\n",
      "[12/13/2025-19:45:18] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/13/2025-19:45:45] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/13/2025-19:45:46] [TRT] [I] Total Host Persistent Memory: 195008 bytes\n",
      "[12/13/2025-19:45:46] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/13/2025-19:45:46] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/13/2025-19:45:46] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 31 steps to complete.\n",
      "[12/13/2025-19:45:46] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.135754ms to assign 3 blocks to 31 nodes requiring 42991616 bytes.\n",
      "[12/13/2025-19:45:46] [TRT] [I] Total Activation Memory: 42991616 bytes\n",
      "[12/13/2025-19:45:46] [TRT] [I] Total Weights Memory: 2928680 bytes\n",
      "[12/13/2025-19:45:46] [TRT] [I] Engine generation completed in 27.5504 seconds.\n",
      "[12/13/2025-19:45:46] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 48 MiB\n",
      "Saved: squeezenet_fp32_b64.engine\n",
      "[12/13/2025-19:45:46] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:45:46] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/13/2025-19:45:46] [TRT] [I] Opset version:    13\n",
      "[12/13/2025-19:45:46] [TRT] [I] Producer name:    pytorch\n",
      "[12/13/2025-19:45:46] [TRT] [I] Producer version: 2.9.1\n",
      "[12/13/2025-19:45:46] [TRT] [I] Domain:           \n",
      "[12/13/2025-19:45:46] [TRT] [I] Model version:    0\n",
      "[12/13/2025-19:45:46] [TRT] [I] Doc string:       \n",
      "[12/13/2025-19:45:46] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:45:46] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +2, now: CPU 1095, GPU 722 (MiB)\n",
      "[12/13/2025-19:45:46] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/13/2025-19:46:13] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/13/2025-19:46:13] [TRT] [I] Total Host Persistent Memory: 195072 bytes\n",
      "[12/13/2025-19:46:13] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/13/2025-19:46:13] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/13/2025-19:46:13] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 31 steps to complete.\n",
      "[12/13/2025-19:46:13] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.134452ms to assign 3 blocks to 31 nodes requiring 85983232 bytes.\n",
      "[12/13/2025-19:46:13] [TRT] [I] Total Activation Memory: 85983232 bytes\n",
      "[12/13/2025-19:46:13] [TRT] [I] Total Weights Memory: 2928680 bytes\n",
      "[12/13/2025-19:46:13] [TRT] [I] Engine generation completed in 26.7736 seconds.\n",
      "[12/13/2025-19:46:13] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 96 MiB\n",
      "Saved: squeezenet_fp32_b128.engine\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "\n",
    "# IMPORTANT: these ONNX files must be exported with FIXED batch sizes (static)\n",
    "onnx_map = {\n",
    "    1:   \"squeezenet_fp32_b1_op13.onnx\",\n",
    "    64:  \"squeezenet_fp32_b64_op13.onnx\",\n",
    "    128: \"squeezenet_fp32_b128_op13.onnx\",\n",
    "}\n",
    "\n",
    "def build_static_engine(onnx_path, engine_path):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "        with open(onnx_path, \"rb\") as f:\n",
    "            if not parser.parse(f.read()):\n",
    "                for i in range(parser.num_errors):\n",
    "                    print(parser.get_error(i))\n",
    "                raise RuntimeError(f\"ONNX parse failed for {onnx_path}\")\n",
    "\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)\n",
    "\n",
    "        # NO optimization profile => static engine (uses whatever fixed shape is in ONNX)\n",
    "        serialized = builder.build_serialized_network(network, config)\n",
    "        if serialized is None:\n",
    "            raise RuntimeError(f\"Engine build failed for {onnx_path}\")\n",
    "\n",
    "        with open(engine_path, \"wb\") as f:\n",
    "            f.write(serialized)\n",
    "\n",
    "    print(\"Saved:\", engine_path)\n",
    "\n",
    "for bs, onnx_path in onnx_map.items():\n",
    "    engine_path = f\"squeezenet_fp32_b{bs}.engine\"\n",
    "    build_static_engine(onnx_path, engine_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37bac398-e290-467b-bf6c-a49cc60b1cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 3.1M Dec 13 19:45 squeezenet_fp32_b1.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 3.4M Dec 13 19:45 squeezenet_fp32_b64.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 3.3M Dec 13 19:46 squeezenet_fp32_b128.engine\n"
     ]
    }
   ],
   "source": [
    "!ls -lh squeezenet_fp32_b1.engine\n",
    "!ls -lh squeezenet_fp32_b64.engine\n",
    "!ls -lh squeezenet_fp32_b128.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa592380-cf3a-43a7-bfdf-19ee4c109118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmark (3 static engines)...\n",
      "squeezenet_fp32_b1.engine | batch=1\n",
      "  latency:     0.230 ms/batch\n",
      "  per-image:   0.229911 ms/image\n",
      "  throughput:  4349.5 images/sec\n",
      "squeezenet_fp32_b64.engine | batch=64\n",
      "  latency:     0.454 ms/batch\n",
      "  per-image:   0.007099 ms/image\n",
      "  throughput:  140871.4 images/sec\n",
      "squeezenet_fp32_b128.engine | batch=128\n",
      "  latency:     0.737 ms/batch\n",
      "  per-image:   0.005760 ms/image\n",
      "  throughput:  173614.3 images/sec\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import torch\n",
    "\n",
    "def benchmark_engine_static(engine_path, batch_size, iters=1000):\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "    with open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
    "    inp = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.INPUT][0]\n",
    "    out = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.OUTPUT][0]\n",
    "\n",
    "    # ✅ STATIC engine: DO NOT set_input_shape()\n",
    "    # context.set_input_shape(inp, (batch_size, 3, 32, 32))\n",
    "\n",
    "    x = torch.randn(batch_size, 3, 32, 32, device=\"cuda\", dtype=torch.float32)\n",
    "    y = torch.empty(tuple(context.get_tensor_shape(out)), device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "    context.set_tensor_address(inp, int(x.data_ptr()))\n",
    "    context.set_tensor_address(out, int(y.data_ptr()))\n",
    "\n",
    "    stream = torch.cuda.Stream()\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(50):\n",
    "        context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end   = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start.record(stream)\n",
    "    for _ in range(iters):\n",
    "        context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "    end.record(stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "    elapsed_ms = start.elapsed_time(end)\n",
    "\n",
    "    batch_latency_ms = elapsed_ms / iters\n",
    "    throughput = (iters * batch_size) / (elapsed_ms / 1000.0)\n",
    "    ms_per_img = batch_latency_ms / batch_size\n",
    "\n",
    "    print(f\"{engine_path} | batch={batch_size}\")\n",
    "    print(f\"  latency:     {batch_latency_ms:.3f} ms/batch\")\n",
    "    print(f\"  per-image:   {ms_per_img:.6f} ms/image\")\n",
    "    print(f\"  throughput:  {throughput:.1f} images/sec\")\n",
    "\n",
    "\n",
    "print(\"Starting benchmark (3 static engines)...\")\n",
    "benchmark_engine_static(\"squeezenet_fp32_b1.engine\",   batch_size=1,   iters=1000)\n",
    "benchmark_engine_static(\"squeezenet_fp32_b64.engine\",  batch_size=64,  iters=1000)\n",
    "benchmark_engine_static(\"squeezenet_fp32_b128.engine\", batch_size=128, iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1dfa043-1872-4920-873f-04560db1663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2025-19:46:15] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:46:15] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/13/2025-19:46:15] [TRT] [I] Opset version:    13\n",
      "[12/13/2025-19:46:15] [TRT] [I] Producer name:    pytorch\n",
      "[12/13/2025-19:46:15] [TRT] [I] Producer version: 2.9.1\n",
      "[12/13/2025-19:46:15] [TRT] [I] Domain:           \n",
      "[12/13/2025-19:46:15] [TRT] [I] Model version:    0\n",
      "[12/13/2025-19:46:15] [TRT] [I] Doc string:       \n",
      "[12/13/2025-19:46:15] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:46:15] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +0, now: CPU 1086, GPU 732 (MiB)\n",
      "[12/13/2025-19:46:15] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/13/2025-19:47:24] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/13/2025-19:47:25] [TRT] [I] Total Host Persistent Memory: 198112 bytes\n",
      "[12/13/2025-19:47:25] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/13/2025-19:47:25] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/13/2025-19:47:25] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 34 steps to complete.\n",
      "[12/13/2025-19:47:25] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.233147ms to assign 3 blocks to 34 nodes requiring 335872 bytes.\n",
      "[12/13/2025-19:47:25] [TRT] [I] Total Activation Memory: 335872 bytes\n",
      "[12/13/2025-19:47:25] [TRT] [I] Total Weights Memory: 1476640 bytes\n",
      "[12/13/2025-19:47:25] [TRT] [I] Engine generation completed in 69.8792 seconds.\n",
      "[12/13/2025-19:47:25] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 96 MiB\n",
      "Saved: squeezenet_fp16_b1.engine\n",
      "[12/13/2025-19:47:25] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:47:25] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/13/2025-19:47:25] [TRT] [I] Opset version:    13\n",
      "[12/13/2025-19:47:25] [TRT] [I] Producer name:    pytorch\n",
      "[12/13/2025-19:47:25] [TRT] [I] Producer version: 2.9.1\n",
      "[12/13/2025-19:47:25] [TRT] [I] Domain:           \n",
      "[12/13/2025-19:47:25] [TRT] [I] Model version:    0\n",
      "[12/13/2025-19:47:25] [TRT] [I] Doc string:       \n",
      "[12/13/2025-19:47:25] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:47:26] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +0, now: CPU 1239, GPU 740 (MiB)\n",
      "[12/13/2025-19:47:26] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/13/2025-19:48:25] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/13/2025-19:48:25] [TRT] [I] Total Host Persistent Memory: 173504 bytes\n",
      "[12/13/2025-19:48:25] [TRT] [I] Total Device Persistent Memory: 5120 bytes\n",
      "[12/13/2025-19:48:25] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/13/2025-19:48:25] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 38 steps to complete.\n",
      "[12/13/2025-19:48:25] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.214141ms to assign 3 blocks to 38 nodes requiring 21495808 bytes.\n",
      "[12/13/2025-19:48:25] [TRT] [I] Total Activation Memory: 21495808 bytes\n",
      "[12/13/2025-19:48:25] [TRT] [I] Total Weights Memory: 1477664 bytes\n",
      "[12/13/2025-19:48:25] [TRT] [I] Engine generation completed in 59.705 seconds.\n",
      "[12/13/2025-19:48:25] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 96 MiB\n",
      "Saved: squeezenet_fp16_b64.engine\n",
      "[12/13/2025-19:48:25] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:48:25] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/13/2025-19:48:25] [TRT] [I] Opset version:    13\n",
      "[12/13/2025-19:48:25] [TRT] [I] Producer name:    pytorch\n",
      "[12/13/2025-19:48:25] [TRT] [I] Producer version: 2.9.1\n",
      "[12/13/2025-19:48:25] [TRT] [I] Domain:           \n",
      "[12/13/2025-19:48:25] [TRT] [I] Model version:    0\n",
      "[12/13/2025-19:48:25] [TRT] [I] Doc string:       \n",
      "[12/13/2025-19:48:25] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/13/2025-19:48:26] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +0, now: CPU 1243, GPU 776 (MiB)\n",
      "[12/13/2025-19:48:26] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/13/2025-19:49:23] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/13/2025-19:49:23] [TRT] [I] Total Host Persistent Memory: 184288 bytes\n",
      "[12/13/2025-19:49:23] [TRT] [I] Total Device Persistent Memory: 1024 bytes\n",
      "[12/13/2025-19:49:23] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/13/2025-19:49:23] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 32 steps to complete.\n",
      "[12/13/2025-19:49:23] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.150902ms to assign 3 blocks to 32 nodes requiring 42991616 bytes.\n",
      "[12/13/2025-19:49:23] [TRT] [I] Total Activation Memory: 42991616 bytes\n",
      "[12/13/2025-19:49:23] [TRT] [I] Total Weights Memory: 1477664 bytes\n",
      "[12/13/2025-19:49:23] [TRT] [I] Engine generation completed in 57.2892 seconds.\n",
      "[12/13/2025-19:49:23] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 96 MiB\n",
      "Saved: squeezenet_fp16_b128.engine\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "\n",
    "# IMPORTANT: these ONNX files must be exported with FIXED batch sizes (static)\n",
    "onnx_map = {\n",
    "    1:   \"squeezenet_fp32_b1_op13.onnx\",\n",
    "    64:  \"squeezenet_fp32_b64_op13.onnx\",\n",
    "    128: \"squeezenet_fp32_b128_op13.onnx\",\n",
    "}\n",
    "\n",
    "def build_static_fp16_engine(onnx_path, engine_path):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "        with open(onnx_path, \"rb\") as f:\n",
    "            if not parser.parse(f.read()):\n",
    "                for i in range(parser.num_errors):\n",
    "                    print(parser.get_error(i))\n",
    "                raise RuntimeError(f\"ONNX parse failed for {onnx_path}\")\n",
    "\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)\n",
    "\n",
    "        # ✅ FP16 enabled\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "\n",
    "        # ✅ NO optimization profile => static engine (fixed shape from ONNX)\n",
    "        serialized = builder.build_serialized_network(network, config)\n",
    "        if serialized is None:\n",
    "            raise RuntimeError(f\"FP16 engine build failed for {onnx_path}\")\n",
    "\n",
    "        with open(engine_path, \"wb\") as f:\n",
    "            f.write(serialized)\n",
    "\n",
    "    print(\"Saved:\", engine_path)\n",
    "\n",
    "for bs, onnx_path in onnx_map.items():\n",
    "    engine_path = f\"squeezenet_fp16_b{bs}.engine\"\n",
    "    build_static_fp16_engine(onnx_path, engine_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dff36bb1-0473-480e-96e4-239d24e4e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 1.8M Dec 13 19:47 squeezenet_fp16_b1.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 1.9M Dec 13 19:48 squeezenet_fp16_b64.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 1.9M Dec 13 19:49 squeezenet_fp16_b128.engine\n"
     ]
    }
   ],
   "source": [
    "!ls -lh squeezenet_fp16_b1.engine\n",
    "!ls -lh squeezenet_fp16_b64.engine\n",
    "!ls -lh squeezenet_fp16_b128.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eb914e1-f19a-42c2-b8e4-04ae245d5766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squeezenet_fp16_b1.engine | batch=1\n",
      "  batch latency: 0.198 ms/batch\n",
      "  per-image:     0.197999 ms/image\n",
      "  throughput:    5050.5 images/sec\n",
      "squeezenet_fp16_b64.engine | batch=64\n",
      "  batch latency: 0.304 ms/batch\n",
      "  per-image:     0.004752 ms/image\n",
      "  throughput:    210439.8 images/sec\n",
      "squeezenet_fp16_b128.engine | batch=128\n",
      "  batch latency: 0.410 ms/batch\n",
      "  per-image:     0.003203 ms/image\n",
      "  throughput:    312212.0 images/sec\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import torch\n",
    "\n",
    "def run_engine_static(engine_path, batch, iters=1000, warmup=50):\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "    with open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    tensor_names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
    "    inp_name  = [n for n in tensor_names if engine.get_tensor_mode(n) == trt.TensorIOMode.INPUT][0]\n",
    "    out_name  = [n for n in tensor_names if engine.get_tensor_mode(n) == trt.TensorIOMode.OUTPUT][0]\n",
    "\n",
    "    # ❌ NO set_input_shape() for static engines\n",
    "\n",
    "    x = torch.randn(batch, 3, 32, 32, device=\"cuda\", dtype=torch.float32)\n",
    "    y = torch.empty(tuple(context.get_tensor_shape(out_name)), device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "    context.set_tensor_address(inp_name, int(x.data_ptr()))\n",
    "    context.set_tensor_address(out_name, int(y.data_ptr()))\n",
    "\n",
    "    stream = torch.cuda.Stream()\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(warmup):\n",
    "        context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end   = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start.record(stream)\n",
    "    for _ in range(iters):\n",
    "        context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "    end.record(stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "    elapsed_ms = start.elapsed_time(end)\n",
    "    batch_latency_ms = elapsed_ms / iters\n",
    "    ms_per_img = batch_latency_ms / batch\n",
    "    img_per_sec = (iters * batch) / (elapsed_ms / 1000.0)\n",
    "\n",
    "    print(f\"{engine_path} | batch={batch}\")\n",
    "    print(f\"  batch latency: {batch_latency_ms:.3f} ms/batch\")\n",
    "    print(f\"  per-image:     {ms_per_img:.6f} ms/image\")\n",
    "    print(f\"  throughput:    {img_per_sec:.1f} images/sec\")\n",
    "\n",
    "run_engine_static(\"squeezenet_fp16_b1.engine\",   1)\n",
    "run_engine_static(\"squeezenet_fp16_b64.engine\",  64)\n",
    "run_engine_static(\"squeezenet_fp16_b128.engine\", 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "250f0dd1-9496-4b88-add3-6ab7503cd722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57.7%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53f3b54e-aac2-4547-af94-14fdf8d9f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader_b1   = DataLoader(test_dataset, batch_size=1,   shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\n",
    "test_loader_b64  = DataLoader(test_dataset, batch_size=64,  shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\n",
    "test_loader_b128 = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcb0ea02-1c28-416f-917b-48b6789cd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "@torch.no_grad()\n",
    "def trt_accuracy_static(engine_path, test_loader, num_batches=None):\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "    with open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
    "    inp = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.INPUT][0]\n",
    "    out = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.OUTPUT][0]\n",
    "\n",
    "    # engine fixed shapes\n",
    "    in_shape = tuple(engine.get_tensor_shape(inp))\n",
    "    out_shape = tuple(engine.get_tensor_shape(out))\n",
    "    fixed_bsz = in_shape[0]  # should be 1 or 64 or 128\n",
    "\n",
    "    # output dtype\n",
    "    trt_dtype = engine.get_tensor_dtype(out)\n",
    "    torch_dtype = {\n",
    "        trt.DataType.FLOAT: torch.float32,\n",
    "        trt.DataType.HALF:  torch.float16,\n",
    "        trt.DataType.INT8:  torch.int8,\n",
    "        trt.DataType.INT32: torch.int32,\n",
    "    }[trt_dtype]\n",
    "\n",
    "    stream = torch.cuda.current_stream()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for bi, (x_cpu, y_cpu) in enumerate(test_loader):\n",
    "        if num_batches is not None and bi >= num_batches:\n",
    "            break\n",
    "\n",
    "        x = x_cpu.to(\"cuda\", non_blocking=True)\n",
    "        y = y_cpu.to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        if x.shape[0] != fixed_bsz:\n",
    "            raise RuntimeError(f\"Batch mismatch: loader={x.shape[0]} but engine expects {fixed_bsz}\")\n",
    "\n",
    "        yhat = torch.empty(out_shape, device=\"cuda\", dtype=torch_dtype)\n",
    "\n",
    "        context.set_tensor_address(inp, int(x.data_ptr()))\n",
    "        context.set_tensor_address(out, int(yhat.data_ptr()))\n",
    "\n",
    "        ok = context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "        if not ok:\n",
    "            raise RuntimeError(\"TRT execute failed\")\n",
    "\n",
    "        pred = yhat.float().argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.shape[0]\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15e04cc8-22e8-41d7-9345-d5bd98464ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2025-19:49:52] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/13/2025-19:50:03] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/13/2025-19:50:04] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "FP32 TRT Acc b1:   89.21%\n",
      "FP32 TRT Acc b64:  89.19%\n",
      "FP32 TRT Acc b128: 89.19%\n"
     ]
    }
   ],
   "source": [
    "acc1   = trt_accuracy_static(\"squeezenet_fp32_b1.engine\",   test_loader_b1)\n",
    "acc64  = trt_accuracy_static(\"squeezenet_fp32_b64.engine\",  test_loader_b64)\n",
    "acc128 = trt_accuracy_static(\"squeezenet_fp32_b128.engine\", test_loader_b128)\n",
    "\n",
    "print(f\"FP32 TRT Acc b1:   {acc1:.2f}%\")\n",
    "print(f\"FP32 TRT Acc b64:  {acc64:.2f}%\")\n",
    "print(f\"FP32 TRT Acc b128: {acc128:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90f820f0-364f-4320-93e4-b90ed0965239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2025-19:50:05] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/13/2025-19:50:16] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/13/2025-19:50:17] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "FP16 TRT Acc b1:   89.23%\n",
      "FP16 TRT Acc b64:  89.20%\n",
      "FP16 TRT Acc b128: 89.21%\n"
     ]
    }
   ],
   "source": [
    "acc1   = trt_accuracy_static(\"squeezenet_fp16_b1.engine\",   test_loader_b1)\n",
    "acc64  = trt_accuracy_static(\"squeezenet_fp16_b64.engine\",  test_loader_b64)\n",
    "acc128 = trt_accuracy_static(\"squeezenet_fp16_b128.engine\", test_loader_b128)\n",
    "\n",
    "print(f\"FP16 TRT Acc b1:   {acc1:.2f}%\")\n",
    "print(f\"FP16 TRT Acc b64:  {acc64:.2f}%\")\n",
    "print(f\"FP16 TRT Acc b128: {acc128:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5be43fb-7d8f-445b-8bc2-27280ac2ce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_5267562/ipykernel_97961/1827212825.py:69: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.\n",
      "  config.int8_calibrator = EntropyCalibrator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: squeezenet_int8_b1.engine (calib_batches=200)\n",
      "Saved: squeezenet_int8_b64.engine (calib_batches=200)\n",
      "Saved: squeezenet_int8_b128.engine (calib_batches=200)\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import torch\n",
    "\n",
    "# ✅ reduce spam\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "# ✅ use your 3 fixed-shape ONNX files\n",
    "onnx_map = {\n",
    "    1:   \"squeezenet_fp32_b1_op13.onnx\",\n",
    "    64:  \"squeezenet_fp32_b64_op13.onnx\",\n",
    "    128: \"squeezenet_fp32_b128_op13.onnx\",\n",
    "}\n",
    "\n",
    "class EntropyCalibrator(trt.IInt8EntropyCalibrator2):\n",
    "    def __init__(self, calib_loader, max_batches=200, cache_file=\"calib.cache\"):\n",
    "        super().__init__()\n",
    "        self.cache_file = cache_file\n",
    "        self.data_iter = iter(calib_loader)\n",
    "        self.max_batches = max_batches\n",
    "        self.batch_count = 0\n",
    "\n",
    "        x0, _ = next(iter(calib_loader))\n",
    "        self.batch_size = x0.shape[0]\n",
    "        self.device_input = torch.empty_like(x0, device=\"cuda\")\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def get_batch(self, names):\n",
    "        if self.batch_count >= self.max_batches:\n",
    "            return None\n",
    "        try:\n",
    "            x, _ = next(self.data_iter)\n",
    "        except StopIteration:\n",
    "            return None\n",
    "\n",
    "        self.batch_count += 1\n",
    "        x = x.to(\"cuda\", non_blocking=True)\n",
    "        self.device_input.resize_(x.shape).copy_(x)\n",
    "        return [int(self.device_input.data_ptr())]\n",
    "\n",
    "    def read_calibration_cache(self):\n",
    "        try:\n",
    "            with open(self.cache_file, \"rb\") as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            return None\n",
    "\n",
    "    def write_calibration_cache(self, cache):\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            f.write(cache)\n",
    "\n",
    "def build_int8_engine_static(onnx_path, engine_path, calib_loader, max_calib_batches=200):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "        with open(onnx_path, \"rb\") as f:\n",
    "            if not parser.parse(f.read()):\n",
    "                for i in range(parser.num_errors):\n",
    "                    print(parser.get_error(i))\n",
    "                raise RuntimeError(f\"ONNX parse failed for {onnx_path}\")\n",
    "\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)\n",
    "\n",
    "        # INT8 PTQ\n",
    "        config.set_flag(trt.BuilderFlag.INT8)\n",
    "        config.int8_calibrator = EntropyCalibrator(\n",
    "            calib_loader,\n",
    "            max_batches=max_calib_batches,\n",
    "            cache_file=engine_path.replace(\".engine\", \".cache\")\n",
    "        )\n",
    "\n",
    "        # ✅ static ONNX => NO optimization profile needed\n",
    "        serialized = builder.build_serialized_network(network, config)\n",
    "        if serialized is None:\n",
    "            raise RuntimeError(f\"INT8 Engine build failed for {onnx_path}\")\n",
    "\n",
    "        with open(engine_path, \"wb\") as f:\n",
    "            f.write(serialized)\n",
    "\n",
    "    print(f\"Saved: {engine_path} (calib_batches={max_calib_batches})\")\n",
    "\n",
    "# ✅ make sure these match the ONNX batch size:\n",
    "calib_loader_map = {\n",
    "    1:   test_loader_b1,\n",
    "    64:  test_loader_b64,\n",
    "    128: test_loader_b128,\n",
    "}\n",
    "\n",
    "CALIB_BATCHES = 200\n",
    "\n",
    "for bs, onnx_path in onnx_map.items():\n",
    "    build_int8_engine_static(\n",
    "        onnx_path=onnx_path,\n",
    "        engine_path=f\"squeezenet_int8_b{bs}.engine\",\n",
    "        calib_loader=calib_loader_map[bs],\n",
    "        max_calib_batches=CALIB_BATCHES\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5e99fb4-c4f8-421f-ac0d-86fd5d08cf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 1.2M Dec 13 19:51 squeezenet_int8_b1.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 1.4M Dec 13 19:53 squeezenet_int8_b64.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 1.3M Dec 13 19:54 squeezenet_int8_b128.engine\n"
     ]
    }
   ],
   "source": [
    "!ls -lh squeezenet_int8_b1.engine\n",
    "!ls -lh squeezenet_int8_b64.engine\n",
    "!ls -lh squeezenet_int8_b128.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50ef2046-109c-45e9-8d6f-aa5cd7eb4e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2025-19:54:44] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/13/2025-19:54:55] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/13/2025-19:54:56] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "INT8 TRT Acc b1:   89.13%\n",
      "INT8 TRT Acc b64:  89.22%\n",
      "INT8 TRT Acc b128: 89.18%\n"
     ]
    }
   ],
   "source": [
    "acc1   = trt_accuracy_static(\"squeezenet_int8_b1.engine\",   test_loader_b1)\n",
    "acc64  = trt_accuracy_static(\"squeezenet_int8_b64.engine\",  test_loader_b64)\n",
    "acc128 = trt_accuracy_static(\"squeezenet_int8_b128.engine\", test_loader_b128)\n",
    "\n",
    "print(f\"INT8 TRT Acc b1:   {acc1:.2f}%\")\n",
    "print(f\"INT8 TRT Acc b64:  {acc64:.2f}%\")\n",
    "print(f\"INT8 TRT Acc b128: {acc128:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54ffe9cf-4ea9-4d56-9285-6c1ef58cee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch acc: 89.22\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def torch_acc(model, loader, device=\"cuda\"):\n",
    "    model.eval().to(device)\n",
    "    correct = total = 0\n",
    "    for x,y in loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        correct += (pred==y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return 100*correct/total\n",
    "\n",
    "print(\"Torch acc:\", torch_acc(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dd04b1e-a34d-4db9-bea1-ae83cd7205b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(test_loader.dataset.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd5c0d-f755-4c20-a1e7-569de0092dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee599 (PyTorch)",
   "language": "python",
   "name": "ee599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
