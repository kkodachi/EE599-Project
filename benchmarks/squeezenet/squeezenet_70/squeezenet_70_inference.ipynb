{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9fb18d4-66d7-421d-bf00-63460b5c60e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.9.1+cu128 CUDA: True\n",
      "TensorRT: 10.14.1.48.post1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())\n",
    "print(\"TensorRT:\", trt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e20c95b9-71ae-476f-9fe5-3997105ecf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parents[1]))\n",
    "\n",
    "from models.squeezenet_model import SqueezeNetCIFAR10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd93386-e7a2-4028-8377-27899c33347a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNetCIFAR10(\n",
       "  (conv1): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fire2): Fire(\n",
       "    (conv1): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fire3): Fire(\n",
       "    (conv1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fire4): Fire(\n",
       "    (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (dropout4): Dropout(p=0.2, inplace=False)\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fire5): Fire(\n",
       "    (conv1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fire6): Fire(\n",
       "    (conv1): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fire7): Fire(\n",
       "    (conv1): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fire8): Fire(\n",
       "    (conv1): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (dropout8): Dropout(p=0.2, inplace=False)\n",
       "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fire9): Fire(\n",
       "    (conv1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv10): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (dropout_final): Dropout(p=0.5, inplace=False)\n",
       "  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SqueezeNetCIFAR10(num_classes=10)\n",
    "model.load_state_dict(\n",
    "    torch.load(\"../../pth/squeezenet_70.pth\", map_location=\"cpu\")\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ea8db1-83f3-4806-8401-14ac8fcedd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_5287623/ipykernel_2231361/1529071125.py:12: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported squeezenet_70_fp32_b1_op13.onnx\n",
      "Exported squeezenet_70_fp32_b64_op13.onnx\n",
      "Exported squeezenet_70_fp32_b128_op13.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "dummy_map = {\n",
    "    1:   torch.randn(1,   3, 32, 32),\n",
    "    64:  torch.randn(64,  3, 32, 32),\n",
    "    128: torch.randn(128, 3, 32, 32),\n",
    "}\n",
    "\n",
    "for bs, dummy in dummy_map.items():\n",
    "    out_path = f\"squeezenet_70_fp32_b{bs}_op13.onnx\"\n",
    "    torch.onnx.export(\n",
    "        model, dummy, out_path,\n",
    "        opset_version=13,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes=None,   # <-- IMPORTANT: static\n",
    "        dynamo=False\n",
    "    )\n",
    "    print(\"Exported\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17143253-186e-4c27-ad4a-c67f1e4d6429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 2.8M Dec 14 09:26 squeezenet_70_fp32_b1_op13.onnx\n",
      "-rw-r--r-- 1 ihsiao ihsiao 2.8M Dec 14 09:26 squeezenet_70_fp32_b64_op13.onnx\n",
      "-rw-r--r-- 1 ihsiao ihsiao 2.8M Dec 14 09:26 squeezenet_70_fp32_b128_op13.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -lh squeezenet_70_fp32_b1_op13.onnx\n",
    "!ls -lh squeezenet_70_fp32_b64_op13.onnx\n",
    "!ls -lh squeezenet_70_fp32_b128_op13.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85a783e-e0c7-4e28-ae74-93c7507e8996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 13)]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "m = onnx.load(\"squeezenet_70_fp32_b1_op13.onnx\")\n",
    "onnx.checker.check_model(m)\n",
    "print([(op.domain, op.version) for op in m.opset_import])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ac89d5-edbc-4178-b9e1-8f408f840f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.14.1.48.post1\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "print(trt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c0c6a83-4e52-4242-aea3-229a1256bc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2025-09:26:57] [TRT] [I] [MemUsageChange] Init CUDA: CPU -23, GPU +0, now: CPU 558, GPU 1354 (MiB)\n",
      "[12/14/2025-09:26:57] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-09:26:57] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/14/2025-09:26:57] [TRT] [I] Opset version:    13\n",
      "[12/14/2025-09:26:57] [TRT] [I] Producer name:    pytorch\n",
      "[12/14/2025-09:26:57] [TRT] [I] Producer version: 2.9.1\n",
      "[12/14/2025-09:26:57] [TRT] [I] Domain:           \n",
      "[12/14/2025-09:26:57] [TRT] [I] Model version:    0\n",
      "[12/14/2025-09:26:57] [TRT] [I] Doc string:       \n",
      "[12/14/2025-09:26:57] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-09:26:57] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +280, GPU +8, now: CPU 673, GPU 1362 (MiB)\n",
      "[12/14/2025-09:26:57] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-09:27:30] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-09:27:30] [TRT] [I] Total Host Persistent Memory: 194496 bytes\n",
      "[12/14/2025-09:27:30] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/14/2025-09:27:30] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/14/2025-09:27:30] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 35 steps to complete.\n",
      "[12/14/2025-09:27:30] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.18606ms to assign 3 blocks to 35 nodes requiring 671744 bytes.\n",
      "[12/14/2025-09:27:30] [TRT] [I] Total Activation Memory: 671744 bytes\n",
      "[12/14/2025-09:27:30] [TRT] [I] Total Weights Memory: 2928680 bytes\n",
      "[12/14/2025-09:27:30] [TRT] [I] Engine generation completed in 33.1285 seconds.\n",
      "[12/14/2025-09:27:30] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 7 MiB\n",
      "Saved: squeezenet_70_fp32_b1.engine\n",
      "[12/14/2025-09:27:30] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-09:27:30] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/14/2025-09:27:30] [TRT] [I] Opset version:    13\n",
      "[12/14/2025-09:27:30] [TRT] [I] Producer name:    pytorch\n",
      "[12/14/2025-09:27:30] [TRT] [I] Producer version: 2.9.1\n",
      "[12/14/2025-09:27:30] [TRT] [I] Domain:           \n",
      "[12/14/2025-09:27:30] [TRT] [I] Model version:    0\n",
      "[12/14/2025-09:27:30] [TRT] [I] Doc string:       \n",
      "[12/14/2025-09:27:30] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-09:27:31] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +2, now: CPU 1075, GPU 1650 (MiB)\n",
      "[12/14/2025-09:27:31] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-09:27:58] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-09:27:58] [TRT] [I] Total Host Persistent Memory: 195008 bytes\n",
      "[12/14/2025-09:27:58] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/14/2025-09:27:58] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/14/2025-09:27:58] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 31 steps to complete.\n",
      "[12/14/2025-09:27:58] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.134223ms to assign 3 blocks to 31 nodes requiring 42991616 bytes.\n",
      "[12/14/2025-09:27:58] [TRT] [I] Total Activation Memory: 42991616 bytes\n",
      "[12/14/2025-09:27:58] [TRT] [I] Total Weights Memory: 2928680 bytes\n",
      "[12/14/2025-09:27:58] [TRT] [I] Engine generation completed in 27.6248 seconds.\n",
      "[12/14/2025-09:27:58] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 48 MiB\n",
      "Saved: squeezenet_70_fp32_b64.engine\n",
      "[12/14/2025-09:27:58] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-09:27:58] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/14/2025-09:27:58] [TRT] [I] Opset version:    13\n",
      "[12/14/2025-09:27:58] [TRT] [I] Producer name:    pytorch\n",
      "[12/14/2025-09:27:58] [TRT] [I] Producer version: 2.9.1\n",
      "[12/14/2025-09:27:58] [TRT] [I] Domain:           \n",
      "[12/14/2025-09:27:58] [TRT] [I] Model version:    0\n",
      "[12/14/2025-09:27:58] [TRT] [I] Doc string:       \n",
      "[12/14/2025-09:27:58] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-09:27:59] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +2, now: CPU 1099, GPU 1654 (MiB)\n",
      "[12/14/2025-09:27:59] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-09:28:25] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-09:28:25] [TRT] [I] Total Host Persistent Memory: 195072 bytes\n",
      "[12/14/2025-09:28:25] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/14/2025-09:28:25] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/14/2025-09:28:25] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 31 steps to complete.\n",
      "[12/14/2025-09:28:25] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.133871ms to assign 3 blocks to 31 nodes requiring 85983232 bytes.\n",
      "[12/14/2025-09:28:25] [TRT] [I] Total Activation Memory: 85983232 bytes\n",
      "[12/14/2025-09:28:25] [TRT] [I] Total Weights Memory: 2928680 bytes\n",
      "[12/14/2025-09:28:25] [TRT] [I] Engine generation completed in 26.7771 seconds.\n",
      "[12/14/2025-09:28:25] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 96 MiB\n",
      "Saved: squeezenet_70_fp32_b128.engine\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "\n",
    "# IMPORTANT: these ONNX files must be exported with FIXED batch sizes (static)\n",
    "onnx_map = {\n",
    "    1:   \"squeezenet_70_fp32_b1_op13.onnx\",\n",
    "    64:  \"squeezenet_70_fp32_b64_op13.onnx\",\n",
    "    128: \"squeezenet_70_fp32_b128_op13.onnx\",\n",
    "}\n",
    "\n",
    "def build_static_engine(onnx_path, engine_path):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "        with open(onnx_path, \"rb\") as f:\n",
    "            if not parser.parse(f.read()):\n",
    "                for i in range(parser.num_errors):\n",
    "                    print(parser.get_error(i))\n",
    "                raise RuntimeError(f\"ONNX parse failed for {onnx_path}\")\n",
    "\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)\n",
    "\n",
    "        # NO optimization profile => static engine (uses whatever fixed shape is in ONNX)\n",
    "        serialized = builder.build_serialized_network(network, config)\n",
    "        if serialized is None:\n",
    "            raise RuntimeError(f\"Engine build failed for {onnx_path}\")\n",
    "\n",
    "        with open(engine_path, \"wb\") as f:\n",
    "            f.write(serialized)\n",
    "\n",
    "    print(\"Saved:\", engine_path)\n",
    "\n",
    "for bs, onnx_path in onnx_map.items():\n",
    "    engine_path = f\"squeezenet_70_fp32_b{bs}.engine\"\n",
    "    build_static_engine(onnx_path, engine_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37bac398-e290-467b-bf6c-a49cc60b1cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 3.1M Dec 14 09:27 squeezenet_70_fp32_b1.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 3.4M Dec 14 09:27 squeezenet_70_fp32_b64.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 3.4M Dec 14 09:28 squeezenet_70_fp32_b128.engine\n"
     ]
    }
   ],
   "source": [
    "!ls -lh squeezenet_70_fp32_b1.engine\n",
    "!ls -lh squeezenet_70_fp32_b64.engine\n",
    "!ls -lh squeezenet_70_fp32_b128.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa592380-cf3a-43a7-bfdf-19ee4c109118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmark (3 static engines)...\n",
      "squeezenet_70_fp32_b1.engine | batch=1\n",
      "  latency:     0.226 ms/batch\n",
      "  per-image:   0.226146 ms/image\n",
      "  throughput:  4421.9 images/sec\n",
      "squeezenet_70_fp32_b64.engine | batch=64\n",
      "  latency:     0.457 ms/batch\n",
      "  per-image:   0.007145 ms/image\n",
      "  throughput:  139949.1 images/sec\n",
      "squeezenet_70_fp32_b128.engine | batch=128\n",
      "  latency:     0.743 ms/batch\n",
      "  per-image:   0.005806 ms/image\n",
      "  throughput:  172244.6 images/sec\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import torch\n",
    "\n",
    "def benchmark_engine_static(engine_path, batch_size, iters=1000):\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "    with open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
    "    inp = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.INPUT][0]\n",
    "    out = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.OUTPUT][0]\n",
    "\n",
    "    # ✅ STATIC engine: DO NOT set_input_shape()\n",
    "    # context.set_input_shape(inp, (batch_size, 3, 32, 32))\n",
    "\n",
    "    x = torch.randn(batch_size, 3, 32, 32, device=\"cuda\", dtype=torch.float32)\n",
    "    y = torch.empty(tuple(context.get_tensor_shape(out)), device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "    context.set_tensor_address(inp, int(x.data_ptr()))\n",
    "    context.set_tensor_address(out, int(y.data_ptr()))\n",
    "\n",
    "    stream = torch.cuda.Stream()\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(50):\n",
    "        context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end   = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start.record(stream)\n",
    "    for _ in range(iters):\n",
    "        context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "    end.record(stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "    elapsed_ms = start.elapsed_time(end)\n",
    "\n",
    "    batch_latency_ms = elapsed_ms / iters\n",
    "    throughput = (iters * batch_size) / (elapsed_ms / 1000.0)\n",
    "    ms_per_img = batch_latency_ms / batch_size\n",
    "\n",
    "    print(f\"{engine_path} | batch={batch_size}\")\n",
    "    print(f\"  latency:     {batch_latency_ms:.3f} ms/batch\")\n",
    "    print(f\"  per-image:   {ms_per_img:.6f} ms/image\")\n",
    "    print(f\"  throughput:  {throughput:.1f} images/sec\")\n",
    "\n",
    "\n",
    "print(\"Starting benchmark (3 static engines)...\")\n",
    "benchmark_engine_static(\"squeezenet_70_fp32_b1.engine\",   batch_size=1,   iters=1000)\n",
    "benchmark_engine_static(\"squeezenet_70_fp32_b64.engine\",  batch_size=64,  iters=1000)\n",
    "benchmark_engine_static(\"squeezenet_70_fp32_b128.engine\", batch_size=128, iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1dfa043-1872-4920-873f-04560db1663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2025-09:28:27] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-09:28:27] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/14/2025-09:28:27] [TRT] [I] Opset version:    13\n",
      "[12/14/2025-09:28:27] [TRT] [I] Producer name:    pytorch\n",
      "[12/14/2025-09:28:27] [TRT] [I] Producer version: 2.9.1\n",
      "[12/14/2025-09:28:27] [TRT] [I] Domain:           \n",
      "[12/14/2025-09:28:27] [TRT] [I] Model version:    0\n",
      "[12/14/2025-09:28:27] [TRT] [I] Doc string:       \n",
      "[12/14/2025-09:28:27] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-09:28:28] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +0, now: CPU 1090, GPU 1664 (MiB)\n",
      "[12/14/2025-09:28:28] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-09:29:38] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-09:29:39] [TRT] [I] Total Host Persistent Memory: 193632 bytes\n",
      "[12/14/2025-09:29:39] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/14/2025-09:29:39] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/14/2025-09:29:39] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 38 steps to complete.\n",
      "[12/14/2025-09:29:39] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.226766ms to assign 3 blocks to 38 nodes requiring 335872 bytes.\n",
      "[12/14/2025-09:29:39] [TRT] [I] Total Activation Memory: 335872 bytes\n",
      "[12/14/2025-09:29:39] [TRT] [I] Total Weights Memory: 1476640 bytes\n",
      "[12/14/2025-09:29:39] [TRT] [I] Engine generation completed in 71.1496 seconds.\n",
      "[12/14/2025-09:29:39] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 96 MiB\n",
      "Saved: squeezenet_70_fp16_b1.engine\n",
      "[12/14/2025-09:29:39] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-09:29:39] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/14/2025-09:29:39] [TRT] [I] Opset version:    13\n",
      "[12/14/2025-09:29:39] [TRT] [I] Producer name:    pytorch\n",
      "[12/14/2025-09:29:39] [TRT] [I] Producer version: 2.9.1\n",
      "[12/14/2025-09:29:39] [TRT] [I] Domain:           \n",
      "[12/14/2025-09:29:39] [TRT] [I] Model version:    0\n",
      "[12/14/2025-09:29:39] [TRT] [I] Doc string:       \n",
      "[12/14/2025-09:29:39] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-09:29:39] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +0, now: CPU 1242, GPU 1672 (MiB)\n",
      "[12/14/2025-09:29:39] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-09:30:39] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-09:30:39] [TRT] [I] Total Host Persistent Memory: 177408 bytes\n",
      "[12/14/2025-09:30:39] [TRT] [I] Total Device Persistent Memory: 5120 bytes\n",
      "[12/14/2025-09:30:39] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/14/2025-09:30:39] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 34 steps to complete.\n",
      "[12/14/2025-09:30:39] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.179247ms to assign 3 blocks to 34 nodes requiring 21495808 bytes.\n",
      "[12/14/2025-09:30:39] [TRT] [I] Total Activation Memory: 21495808 bytes\n",
      "[12/14/2025-09:30:39] [TRT] [I] Total Weights Memory: 1476640 bytes\n",
      "[12/14/2025-09:30:39] [TRT] [I] Engine generation completed in 59.7041 seconds.\n",
      "[12/14/2025-09:30:39] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 96 MiB\n",
      "Saved: squeezenet_70_fp16_b64.engine\n",
      "[12/14/2025-09:30:39] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-09:30:39] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[12/14/2025-09:30:39] [TRT] [I] Opset version:    13\n",
      "[12/14/2025-09:30:39] [TRT] [I] Producer name:    pytorch\n",
      "[12/14/2025-09:30:39] [TRT] [I] Producer version: 2.9.1\n",
      "[12/14/2025-09:30:39] [TRT] [I] Domain:           \n",
      "[12/14/2025-09:30:39] [TRT] [I] Model version:    0\n",
      "[12/14/2025-09:30:39] [TRT] [I] Doc string:       \n",
      "[12/14/2025-09:30:39] [TRT] [I] ----------------------------------------------------------------\n",
      "[12/14/2025-09:30:39] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -277, GPU +0, now: CPU 1241, GPU 1708 (MiB)\n",
      "[12/14/2025-09:30:39] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2025-09:31:36] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[12/14/2025-09:31:37] [TRT] [I] Total Host Persistent Memory: 188832 bytes\n",
      "[12/14/2025-09:31:37] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[12/14/2025-09:31:37] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[12/14/2025-09:31:37] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 34 steps to complete.\n",
      "[12/14/2025-09:31:37] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.175069ms to assign 3 blocks to 34 nodes requiring 42991616 bytes.\n",
      "[12/14/2025-09:31:37] [TRT] [I] Total Activation Memory: 42991616 bytes\n",
      "[12/14/2025-09:31:37] [TRT] [I] Total Weights Memory: 1477664 bytes\n",
      "[12/14/2025-09:31:37] [TRT] [I] Engine generation completed in 57.3599 seconds.\n",
      "[12/14/2025-09:31:37] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 96 MiB\n",
      "Saved: squeezenet_70_fp16_b128.engine\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "\n",
    "# IMPORTANT: these ONNX files must be exported with FIXED batch sizes (static)\n",
    "onnx_map = {\n",
    "    1:   \"squeezenet_70_fp32_b1_op13.onnx\",\n",
    "    64:  \"squeezenet_70_fp32_b64_op13.onnx\",\n",
    "    128: \"squeezenet_70_fp32_b128_op13.onnx\",\n",
    "}\n",
    "\n",
    "def build_static_fp16_engine(onnx_path, engine_path):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "        with open(onnx_path, \"rb\") as f:\n",
    "            if not parser.parse(f.read()):\n",
    "                for i in range(parser.num_errors):\n",
    "                    print(parser.get_error(i))\n",
    "                raise RuntimeError(f\"ONNX parse failed for {onnx_path}\")\n",
    "\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)\n",
    "\n",
    "        # ✅ FP16 enabled\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "\n",
    "        # ✅ NO optimization profile => static engine (fixed shape from ONNX)\n",
    "        serialized = builder.build_serialized_network(network, config)\n",
    "        if serialized is None:\n",
    "            raise RuntimeError(f\"FP16 engine build failed for {onnx_path}\")\n",
    "\n",
    "        with open(engine_path, \"wb\") as f:\n",
    "            f.write(serialized)\n",
    "\n",
    "    print(\"Saved:\", engine_path)\n",
    "\n",
    "for bs, onnx_path in onnx_map.items():\n",
    "    engine_path = f\"squeezenet_70_fp16_b{bs}.engine\"\n",
    "    build_static_fp16_engine(onnx_path, engine_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dff36bb1-0473-480e-96e4-239d24e4e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 1.7M Dec 14 09:29 squeezenet_70_fp16_b1.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 1.9M Dec 14 09:30 squeezenet_70_fp16_b64.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 2.0M Dec 14 09:31 squeezenet_70_fp16_b128.engine\n"
     ]
    }
   ],
   "source": [
    "!ls -lh squeezenet_70_fp16_b1.engine\n",
    "!ls -lh squeezenet_70_fp16_b64.engine\n",
    "!ls -lh squeezenet_70_fp16_b128.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eb914e1-f19a-42c2-b8e4-04ae245d5766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squeezenet_70_fp16_b1.engine | batch=1\n",
      "  batch latency: 0.197 ms/batch\n",
      "  per-image:     0.197310 ms/image\n",
      "  throughput:    5068.2 images/sec\n",
      "squeezenet_70_fp16_b64.engine | batch=64\n",
      "  batch latency: 0.310 ms/batch\n",
      "  per-image:     0.004851 ms/image\n",
      "  throughput:    206142.7 images/sec\n",
      "squeezenet_70_fp16_b128.engine | batch=128\n",
      "  batch latency: 0.412 ms/batch\n",
      "  per-image:     0.003217 ms/image\n",
      "  throughput:    310892.7 images/sec\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import torch\n",
    "\n",
    "def run_engine_static(engine_path, batch, iters=1000, warmup=50):\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "    with open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    tensor_names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
    "    inp_name  = [n for n in tensor_names if engine.get_tensor_mode(n) == trt.TensorIOMode.INPUT][0]\n",
    "    out_name  = [n for n in tensor_names if engine.get_tensor_mode(n) == trt.TensorIOMode.OUTPUT][0]\n",
    "\n",
    "    # ❌ NO set_input_shape() for static engines\n",
    "\n",
    "    x = torch.randn(batch, 3, 32, 32, device=\"cuda\", dtype=torch.float32)\n",
    "    y = torch.empty(tuple(context.get_tensor_shape(out_name)), device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "    context.set_tensor_address(inp_name, int(x.data_ptr()))\n",
    "    context.set_tensor_address(out_name, int(y.data_ptr()))\n",
    "\n",
    "    stream = torch.cuda.Stream()\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(warmup):\n",
    "        context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end   = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start.record(stream)\n",
    "    for _ in range(iters):\n",
    "        context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "    end.record(stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "    elapsed_ms = start.elapsed_time(end)\n",
    "    batch_latency_ms = elapsed_ms / iters\n",
    "    ms_per_img = batch_latency_ms / batch\n",
    "    img_per_sec = (iters * batch) / (elapsed_ms / 1000.0)\n",
    "\n",
    "    print(f\"{engine_path} | batch={batch}\")\n",
    "    print(f\"  batch latency: {batch_latency_ms:.3f} ms/batch\")\n",
    "    print(f\"  per-image:     {ms_per_img:.6f} ms/image\")\n",
    "    print(f\"  throughput:    {img_per_sec:.1f} images/sec\")\n",
    "\n",
    "run_engine_static(\"squeezenet_70_fp16_b1.engine\",   1)\n",
    "run_engine_static(\"squeezenet_70_fp16_b64.engine\",  64)\n",
    "run_engine_static(\"squeezenet_70_fp16_b128.engine\", 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "250f0dd1-9496-4b88-add3-6ab7503cd722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57.7%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53f3b54e-aac2-4547-af94-14fdf8d9f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader_b1   = DataLoader(test_dataset, batch_size=1,   shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\n",
    "test_loader_b64  = DataLoader(test_dataset, batch_size=64,  shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\n",
    "test_loader_b128 = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcb0ea02-1c28-416f-917b-48b6789cd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorrt as trt\n",
    "\n",
    "@torch.no_grad()\n",
    "def trt_accuracy_static(engine_path, test_loader, num_batches=None):\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "    with open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
    "    inp = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.INPUT][0]\n",
    "    out = [n for n in names if engine.get_tensor_mode(n) == trt.TensorIOMode.OUTPUT][0]\n",
    "\n",
    "    # engine fixed shapes\n",
    "    in_shape = tuple(engine.get_tensor_shape(inp))\n",
    "    out_shape = tuple(engine.get_tensor_shape(out))\n",
    "    fixed_bsz = in_shape[0]  # should be 1 or 64 or 128\n",
    "\n",
    "    # output dtype\n",
    "    trt_dtype = engine.get_tensor_dtype(out)\n",
    "    torch_dtype = {\n",
    "        trt.DataType.FLOAT: torch.float32,\n",
    "        trt.DataType.HALF:  torch.float16,\n",
    "        trt.DataType.INT8:  torch.int8,\n",
    "        trt.DataType.INT32: torch.int32,\n",
    "    }[trt_dtype]\n",
    "\n",
    "    stream = torch.cuda.current_stream()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for bi, (x_cpu, y_cpu) in enumerate(test_loader):\n",
    "        if num_batches is not None and bi >= num_batches:\n",
    "            break\n",
    "\n",
    "        x = x_cpu.to(\"cuda\", non_blocking=True)\n",
    "        y = y_cpu.to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        if x.shape[0] != fixed_bsz:\n",
    "            raise RuntimeError(f\"Batch mismatch: loader={x.shape[0]} but engine expects {fixed_bsz}\")\n",
    "\n",
    "        yhat = torch.empty(out_shape, device=\"cuda\", dtype=torch_dtype)\n",
    "\n",
    "        context.set_tensor_address(inp, int(x.data_ptr()))\n",
    "        context.set_tensor_address(out, int(yhat.data_ptr()))\n",
    "\n",
    "        ok = context.execute_async_v3(stream_handle=stream.cuda_stream)\n",
    "        if not ok:\n",
    "            raise RuntimeError(\"TRT execute failed\")\n",
    "\n",
    "        pred = yhat.float().argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.shape[0]\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15e04cc8-22e8-41d7-9345-d5bd98464ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2025-09:31:52] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/14/2025-09:31:59] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/14/2025-09:32:00] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "FP32 TRT Acc b1:   89.71%\n",
      "FP32 TRT Acc b64:  89.72%\n",
      "FP32 TRT Acc b128: 89.71%\n"
     ]
    }
   ],
   "source": [
    "acc1   = trt_accuracy_static(\"squeezenet_70_fp32_b1.engine\",   test_loader_b1)\n",
    "acc64  = trt_accuracy_static(\"squeezenet_70_fp32_b64.engine\",  test_loader_b64)\n",
    "acc128 = trt_accuracy_static(\"squeezenet_70_fp32_b128.engine\", test_loader_b128)\n",
    "\n",
    "print(f\"FP32 TRT Acc b1:   {acc1:.2f}%\")\n",
    "print(f\"FP32 TRT Acc b64:  {acc64:.2f}%\")\n",
    "print(f\"FP32 TRT Acc b128: {acc128:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90f820f0-364f-4320-93e4-b90ed0965239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2025-09:32:01] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/14/2025-09:32:08] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/14/2025-09:32:09] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "FP16 TRT Acc b1:   89.71%\n",
      "FP16 TRT Acc b64:  89.72%\n",
      "FP16 TRT Acc b128: 89.67%\n"
     ]
    }
   ],
   "source": [
    "acc1   = trt_accuracy_static(\"squeezenet_70_fp16_b1.engine\",   test_loader_b1)\n",
    "acc64  = trt_accuracy_static(\"squeezenet_70_fp16_b64.engine\",  test_loader_b64)\n",
    "acc128 = trt_accuracy_static(\"squeezenet_70_fp16_b128.engine\", test_loader_b128)\n",
    "\n",
    "print(f\"FP16 TRT Acc b1:   {acc1:.2f}%\")\n",
    "print(f\"FP16 TRT Acc b64:  {acc64:.2f}%\")\n",
    "print(f\"FP16 TRT Acc b128: {acc128:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5be43fb-7d8f-445b-8bc2-27280ac2ce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_5287623/ipykernel_2231361/3218253646.py:69: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.\n",
      "  config.int8_calibrator = EntropyCalibrator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: squeezenet_70_int8_b1.engine (calib_batches=200)\n",
      "Saved: squeezenet_70_int8_b64.engine (calib_batches=200)\n",
      "Saved: squeezenet_70_int8_b128.engine (calib_batches=200)\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import torch\n",
    "\n",
    "# ✅ reduce spam\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "# ✅ use your 3 fixed-shape ONNX files\n",
    "onnx_map = {\n",
    "    1:   \"squeezenet_70_fp32_b1_op13.onnx\",\n",
    "    64:  \"squeezenet_70_fp32_b64_op13.onnx\",\n",
    "    128: \"squeezenet_70_fp32_b128_op13.onnx\",\n",
    "}\n",
    "\n",
    "class EntropyCalibrator(trt.IInt8EntropyCalibrator2):\n",
    "    def __init__(self, calib_loader, max_batches=200, cache_file=\"calib.cache\"):\n",
    "        super().__init__()\n",
    "        self.cache_file = cache_file\n",
    "        self.data_iter = iter(calib_loader)\n",
    "        self.max_batches = max_batches\n",
    "        self.batch_count = 0\n",
    "\n",
    "        x0, _ = next(iter(calib_loader))\n",
    "        self.batch_size = x0.shape[0]\n",
    "        self.device_input = torch.empty_like(x0, device=\"cuda\")\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def get_batch(self, names):\n",
    "        if self.batch_count >= self.max_batches:\n",
    "            return None\n",
    "        try:\n",
    "            x, _ = next(self.data_iter)\n",
    "        except StopIteration:\n",
    "            return None\n",
    "\n",
    "        self.batch_count += 1\n",
    "        x = x.to(\"cuda\", non_blocking=True)\n",
    "        self.device_input.resize_(x.shape).copy_(x)\n",
    "        return [int(self.device_input.data_ptr())]\n",
    "\n",
    "    def read_calibration_cache(self):\n",
    "        try:\n",
    "            with open(self.cache_file, \"rb\") as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            return None\n",
    "\n",
    "    def write_calibration_cache(self, cache):\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            f.write(cache)\n",
    "\n",
    "def build_int8_engine_static(onnx_path, engine_path, calib_loader, max_calib_batches=200):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "\n",
    "        with open(onnx_path, \"rb\") as f:\n",
    "            if not parser.parse(f.read()):\n",
    "                for i in range(parser.num_errors):\n",
    "                    print(parser.get_error(i))\n",
    "                raise RuntimeError(f\"ONNX parse failed for {onnx_path}\")\n",
    "\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)\n",
    "\n",
    "        # INT8 PTQ\n",
    "        config.set_flag(trt.BuilderFlag.INT8)\n",
    "        config.int8_calibrator = EntropyCalibrator(\n",
    "            calib_loader,\n",
    "            max_batches=max_calib_batches,\n",
    "            cache_file=engine_path.replace(\".engine\", \".cache\")\n",
    "        )\n",
    "\n",
    "        # ✅ static ONNX => NO optimization profile needed\n",
    "        serialized = builder.build_serialized_network(network, config)\n",
    "        if serialized is None:\n",
    "            raise RuntimeError(f\"INT8 Engine build failed for {onnx_path}\")\n",
    "\n",
    "        with open(engine_path, \"wb\") as f:\n",
    "            f.write(serialized)\n",
    "\n",
    "    print(f\"Saved: {engine_path} (calib_batches={max_calib_batches})\")\n",
    "\n",
    "# ✅ make sure these match the ONNX batch size:\n",
    "calib_loader_map = {\n",
    "    1:   test_loader_b1,\n",
    "    64:  test_loader_b64,\n",
    "    128: test_loader_b128,\n",
    "}\n",
    "\n",
    "CALIB_BATCHES = 200\n",
    "\n",
    "for bs, onnx_path in onnx_map.items():\n",
    "    build_int8_engine_static(\n",
    "        onnx_path=onnx_path,\n",
    "        engine_path=f\"squeezenet_70_int8_b{bs}.engine\",\n",
    "        calib_loader=calib_loader_map[bs],\n",
    "        max_calib_batches=CALIB_BATCHES\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5e99fb4-c4f8-421f-ac0d-86fd5d08cf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ihsiao ihsiao 1.2M Dec 14 09:33 squeezenet_70_int8_b1.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 1.4M Dec 14 09:35 squeezenet_70_int8_b64.engine\n",
      "-rw-r--r-- 1 ihsiao ihsiao 1.3M Dec 14 09:36 squeezenet_70_int8_b128.engine\n"
     ]
    }
   ],
   "source": [
    "!ls -lh squeezenet_70_int8_b1.engine\n",
    "!ls -lh squeezenet_70_int8_b64.engine\n",
    "!ls -lh squeezenet_70_int8_b128.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50ef2046-109c-45e9-8d6f-aa5cd7eb4e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2025-09:36:37] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/14/2025-09:36:44] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[12/14/2025-09:36:45] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "INT8 TRT Acc b1:   89.75%\n",
      "INT8 TRT Acc b64:  89.71%\n",
      "INT8 TRT Acc b128: 89.78%\n"
     ]
    }
   ],
   "source": [
    "acc1   = trt_accuracy_static(\"squeezenet_70_int8_b1.engine\",   test_loader_b1)\n",
    "acc64  = trt_accuracy_static(\"squeezenet_70_int8_b64.engine\",  test_loader_b64)\n",
    "acc128 = trt_accuracy_static(\"squeezenet_70_int8_b128.engine\", test_loader_b128)\n",
    "\n",
    "print(f\"INT8 TRT Acc b1:   {acc1:.2f}%\")\n",
    "print(f\"INT8 TRT Acc b64:  {acc64:.2f}%\")\n",
    "print(f\"INT8 TRT Acc b128: {acc128:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54ffe9cf-4ea9-4d56-9285-6c1ef58cee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch acc: 89.69\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def torch_acc(model, loader, device=\"cuda\"):\n",
    "    model.eval().to(device)\n",
    "    correct = total = 0\n",
    "    for x,y in loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        correct += (pred==y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return 100*correct/total\n",
    "\n",
    "print(\"Torch acc:\", torch_acc(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dd04b1e-a34d-4db9-bea1-ae83cd7205b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(test_loader.dataset.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd5c0d-f755-4c20-a1e7-569de0092dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee599 (PyTorch)",
   "language": "python",
   "name": "ee599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
